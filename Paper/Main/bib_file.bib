%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for cheesema at 2018-02-14 09:29:18 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{guignard2017contact,
	Author = {Guignard, Leo and Fiuza, Ulla-Maj and Leggio, Bruno and Faure, Emmanuel and Laussu, Julien and Hufnagel, Lars and Malandain, Gregoire and Godin, Christophe and Lemaire, Patrick},
	Date-Added = {2018-02-14 08:27:14 +0000},
	Date-Modified = {2018-02-14 08:27:23 +0000},
	Journal = {bioRxiv},
	Keywords = {Phallusia},
	Pages = {238741},
	Publisher = {Cold Spring Harbor Laboratory},
	Title = {Contact-dependent cell communications drive morphological invariance during ascidian embryogenesis},
	Year = {2017}}

@url{gunter2017scenery,
	Annote = {@misc{scenery0231github,
  author       = {G{\"u}nther, Harrington, Weigert, Gupta and Sbalzarini}
  title        = {scenerygraphics/scenery: scenery 0.2.3-1},
  month        = dec,
  year         = 2017,
  doi          = {10.5281/zenodo.1111824},
  url          = {https://doi.org/10.5281/zenodo.1111824}
}},
	Date-Added = {2018-02-08 08:12:57 +0000},
	Date-Modified = {2018-02-08 08:14:38 +0000},
	Howpublished = {https://doi.org/10.5281/zenodo.1111824},
	Keywords = {U G{\"u}nther, K Harrington, M Weigert, ? Gupta and I Sbalzarini},
	Month = {dec},
	Title = {scenerygraphics/scenery: scenery 0.2.3-1},
	Year = {2017}}

@article{lenard2015endothelial,
	Author = {Lenard, Anna and Daetwyler, Stephan and Betz, Charles and Ellertsdottir, Elin and Belting, Heinz-Georg and Huisken, Jan and Affolter, Markus},
	Date-Added = {2018-01-30 10:46:26 +0000},
	Date-Modified = {2018-01-30 10:46:26 +0000},
	Journal = {PLoS biology},
	Number = {4},
	Pages = {e1002126},
	Publisher = {Public Library of Science},
	Title = {Endothelial cell self-fusion during vascular pruning},
	Volume = {13},
	Year = {2015}}

@article{daetwyler2016fast,
	Author = {Daetwyler, Stephan and Huisken, Jan},
	Date-Added = {2018-01-30 10:45:08 +0000},
	Date-Modified = {2018-01-30 10:45:08 +0000},
	Journal = {The Biological Bulletin},
	Number = {1},
	Pages = {14--25},
	Publisher = {University of Chicago Press Chicago, IL},
	Title = {Fast fluorescence microscopy with light sheets},
	Volume = {231},
	Year = {2016}}

@article{goldberg2005open,
	Author = {Goldberg, Ilya G and Allan, Chris and Burel, Jean-Marie and Creager, Doug and Falconi, Andrea and Hochheiser, Harry and Johnston, Josiah and Mellen, Jeff and Sorger, Peter K and Swedlow, Jason R},
	Date-Added = {2018-01-24 10:11:16 +0000},
	Date-Modified = {2018-01-24 10:11:16 +0000},
	Journal = {Genome biology},
	Number = {5},
	Pages = {R47},
	Publisher = {BioMed Central},
	Title = {The Open Microscopy Environment (OME) Data Model and XML file: open tools for informatics and quantitative analysis in biological imaging},
	Volume = {6},
	Year = {2005}}

@article{weigert2017care,
	Abstract = {Fluorescence microscopy is a key driver of discoveries in the life-sciences, with observable phenomena being limited by the optics of the microscope, the chemistry of the fluorophores, and the maximum photon exposure tolerated by the sample. These limits necessitate trade-offs between imaging speed, spatial resolution, light exposure, and imaging depth. In this work we show how deep learning enables biological observations beyond the physical limitations of microscopes. On seven concrete examples we illustrate how microscopy images can be restored even if 60-fold fewer photons are used during acquisition, how isotropic resolution can be achieved even with a 10-fold under-sampling along the axial direction, and how diffraction-limited structures can be resolved at 20-times higher frame-rates compared to state-of-the-art methods. All developed image restoration methods are freely available as open source software.},
	Author = {Weigert, Martin and Schmidt, Uwe and Boothe, Tobias and M\uuml;ller, Andreas and Dibrov, Alexander and Jain, Akanksha and Wilhelm, Benjamin and Schmidt, Deborah and Broaddus, Coleman and Culley, Si{\^a}n and Rocha-Martins, Maur{\'\i}cio and Segovia-Miranda, Fabi{\'a}n and Norden, Caren and Henriques, Ricardo and Zerial, Marino and Solimena, Michele and Rink, Jochen and Tomancak, Pavel and Royer, Loic and Jug, Florian and Myers, Eugene W.},
	Date-Added = {2018-01-24 10:08:55 +0000},
	Date-Modified = {2018-01-24 10:09:11 +0000},
	Doi = {10.1101/236463},
	Eprint = {https://www.biorxiv.org/content/early/2017/12/19/236463.full.pdf},
	Journal = {bioRxiv},
	Publisher = {Cold Spring Harbor Laboratory},
	Title = {Content-Aware Image Restoration: Pushing the Limits of Fluorescence Microscopy},
	Url = {https://www.biorxiv.org/content/early/2017/12/19/236463},
	Year = {2017},
	Bdsk-Url-1 = {https://www.biorxiv.org/content/early/2017/12/19/236463},
	Bdsk-Url-2 = {https://dx.doi.org/10.1101/236463}}

@article{monaghan1982particle,
	Author = {Monaghan, Joe J},
	Date-Added = {2018-01-24 10:05:08 +0000},
	Date-Modified = {2018-01-24 10:05:08 +0000},
	Journal = {SIAM Journal on Scientific and Statistical Computing},
	Number = {4},
	Pages = {422--433},
	Publisher = {SIAM},
	Title = {Why particle methods work},
	Volume = {3},
	Year = {1982}}

@article{balazs2017real,
	Author = {Balazs, Balint and Deschamps, Joran and Albert, Marvin and Ries, Jonas and Hufnagel, Lars},
	Date-Added = {2017-12-19 13:39:34 +0000},
	Date-Modified = {2017-12-19 13:39:43 +0000},
	Journal = {bioRxiv},
	Keywords = {Compression, wavelets},
	Pages = {164624},
	Publisher = {Cold Spring Harbor Laboratory},
	Title = {A real-time compression library for microscopy images},
	Year = {2017}}

@inproceedings{liu2010parallel,
	Author = {Liu, Jiangyu and Sun, Jian},
	Booktitle = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
	Date-Added = {2017-11-10 10:39:34 +0000},
	Date-Modified = {2017-11-10 10:39:44 +0000},
	Keywords = {graphcuts},
	Organization = {IEEE},
	Pages = {2181--2188},
	Title = {Parallel graph-cuts by adaptive bottom-up merging},
	Year = {2010}}

@article{meagher1982geometric,
	Author = {Meagher, Donald},
	Date-Added = {2017-11-02 09:14:08 +0000},
	Date-Modified = {2017-11-02 09:14:14 +0000},
	Journal = {Computer graphics and image processing},
	Keywords = {octree},
	Number = {2},
	Pages = {129--147},
	Publisher = {Elsevier},
	Title = {Geometric modeling using octree encoding},
	Volume = {19},
	Year = {1982}}

@article{haar1910theorie,
	Author = {Haar, Alfred},
	Date-Added = {2017-11-02 09:03:38 +0000},
	Date-Modified = {2017-11-02 09:03:47 +0000},
	Journal = {Mathematische Annalen},
	Keywords = {Background Wavelets},
	Number = {3},
	Pages = {331--371},
	Publisher = {Springer},
	Title = {Zur theorie der orthogonalen funktionensysteme},
	Volume = {69},
	Year = {1910}}

@url{3Dseg,
	Date-Added = {2017-08-10 18:04:10 +0000},
	Date-Modified = {2017-08-10 18:04:42 +0000},
	Keywords = {mesh},
	Title = {Repository of static 3D-meshes},
	Url = {http://193.48.251.101:8080/3dsegbenchmark/dataset.html},
	Bdsk-Url-1 = {http://193.48.251.101:8080/3dsegbenchmark/dataset.html}}

@webpage{min2017binvox,
	Author = {Min, Patrick},
	Date-Added = {2017-08-08 12:48:33 +0000},
	Date-Modified = {2017-08-08 12:49:19 +0000},
	Title = {binvox, 3D mesh voxelizer},
	Url = {http://www.patrickmin.com/binvox/},
	Year = {2017},
	Bdsk-Url-1 = {http://www.patrickmin.com/binvox/}}

@article{schmidt2013eureqa,
	Author = {Schmidt, Michael and Lipson, Hod},
	Date-Added = {2017-08-08 11:38:07 +0000},
	Date-Modified = {2017-08-10 18:00:57 +0000},
	Journal = {Nutonian, Somerville, Mass, USA},
	Keywords = {Eureqa},
	Title = {Eureqa},
	Year = {2013}}

@article{davis2015humans,
	Author = {Davis, James and Hsieh, Yi-Hsuan and Lee, Hung-Chi},
	Date-Added = {2017-08-07 10:45:06 +0000},
	Date-Modified = {2017-08-07 10:45:28 +0000},
	Journal = {Scientific reports},
	Keywords = {VisualSystem;eye_huristic;EyeFPS},
	Publisher = {Nature Publishing Group},
	Title = {Humans perceive flicker artifacts at 500 Hz},
	Volume = {5},
	Year = {2015}}

@url{eyeref,
	Author = {Roger N. Clark},
	Date-Added = {2017-08-07 10:32:13 +0000},
	Date-Modified = {2017-08-07 16:45:23 +0000},
	Keywords = {eye_huristic},
	Title = {Notes on the Resolution and Other Details of the Human Eye},
	Url = {http://www.clarkvision.com/articles/eye-resolution.html},
	Year = {2016},
	Bdsk-Url-1 = {http://www.clarkvision.com/articles/eye-resolution.html}}

@webpage{openmp4,
	Author = {OpenMP Architecture Review Board},
	Date-Added = {2017-08-07 10:26:30 +0000},
	Date-Modified = {2017-08-07 10:28:39 +0000},
	Url = {http://www.openmp.org/wp-content/uploads/OpenMP4.0.0.pdf},
	Year = {2013},
	Bdsk-Url-1 = {http://www.openmp.org/wp-content/uploads/OpenMP4.0.0.pdf}}

@online{hdf52017,
	Author = {{The HDF Group}},
	Date-Added = {2017-08-07 10:10:12 +0000},
	Date-Modified = {2017-08-07 10:10:35 +0000},
	Keywords = {HDF5},
	Note = {http://www.hdfgroup.org/HDF5/},
	Title = {{Hierarchical Data Format, version 5}},
	Year = {1997-2017}}

@webpage{alted2017blosc,
	Author = {Alted, F},
	Date-Added = {2017-08-07 10:03:09 +0000},
	Date-Modified = {2017-08-07 10:07:12 +0000},
	Keywords = {BLOSC},
	Title = {Blosc, an extremely fast, multi-threaded, meta-compressor library},
	Url = {https://github.com/Blosc/hdf5-blosc},
	Year = {2017},
	Bdsk-Url-1 = {https://github.com/Blosc/hdf5-blosc}}

@book{goodfellow2016deep,
	Author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	Date-Added = {2017-08-06 13:53:29 +0000},
	Date-Modified = {2017-08-06 13:53:36 +0000},
	Keywords = {DeepLearning},
	Publisher = {MIT press},
	Title = {Deep learning},
	Year = {2016}}

@article{sbalzarini2006ppm,
	Author = {Sbalzarini, Ivo F and Walther, Jens H and Bergdorf, Michael and Hieber, Simone Elke and Kotsalis, Evangelos M and Koumoutsakos, Petros},
	Date-Added = {2017-08-06 12:49:13 +0000},
	Date-Modified = {2017-08-06 12:49:18 +0000},
	Journal = {Journal of Computational Physics},
	Keywords = {PPM},
	Number = {2},
	Pages = {566--588},
	Publisher = {Elsevier},
	Title = {PPM--A highly efficient parallel particle--mesh library for the simulation of continuum systems},
	Volume = {215},
	Year = {2006}}

@inproceedings{defferrard2016convolutional,
	Author = {Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2017-08-06 12:48:08 +0000},
	Date-Modified = {2017-08-06 12:48:21 +0000},
	Keywords = {DeepLearning},
	Pages = {3844--3852},
	Title = {Convolutional neural networks on graphs with fast localized spectral filtering},
	Year = {2016}}

@article{ayachit2015paraview,
	Author = {Ayachit, Utkarsh},
	Date-Added = {2017-07-27 14:21:10 +0000},
	Date-Modified = {2017-07-27 14:21:16 +0000},
	Keywords = {Paraview},
	Publisher = {Kitware, Inc.},
	Title = {The paraview guide: a parallel visualization application},
	Year = {2015}}

@article{dice1945measures,
	Author = {Dice, Lee R},
	Date-Added = {2017-07-26 14:18:31 +0000},
	Date-Modified = {2017-07-26 14:18:55 +0000},
	Journal = {Ecology},
	Keywords = {dice},
	Number = {3},
	Pages = {297--302},
	Publisher = {Wiley Online Library},
	Title = {Measures of the amount of ecologic association between species},
	Volume = {26},
	Year = {1945}}

@book{mathews1970mathematical,
	Author = {Mathews, Jon and Walker, Robert Lee},
	Date-Added = {2017-07-23 09:27:10 +0000},
	Date-Modified = {2017-07-23 09:27:18 +0000},
	Keywords = {Noise},
	Publisher = {WA Benjamin New York},
	Title = {Mathematical methods of physics},
	Volume = {501},
	Year = {1970}}

@article{boykov2004experimental,
	Author = {Boykov, Yuri and Kolmogorov, Vladimir},
	Date-Added = {2017-07-19 13:35:33 +0000},
	Date-Modified = {2017-07-19 13:35:39 +0000},
	Journal = {IEEE transactions on pattern analysis and machine intelligence},
	Keywords = {Implimentation},
	Number = {9},
	Pages = {1124--1137},
	Publisher = {IEEE},
	Title = {An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision},
	Volume = {26},
	Year = {2004}}

@article{nooruddin2003simplification,
	Author = {Nooruddin, Fakir S. and Turk, Greg},
	Date-Added = {2017-07-19 13:35:05 +0000},
	Date-Modified = {2017-07-19 13:35:21 +0000},
	Journal = {IEEE Transactions on Visualization and Computer Graphics},
	Keywords = {Implimentation},
	Number = {2},
	Pages = {191--205},
	Publisher = {IEEE},
	Title = {Simplification and repair of polygonal models using volumetric techniques},
	Volume = {9},
	Year = {2003}}

@misc{sage2002,
	Author = {Sage, Daniel},
	Date-Added = {2017-07-19 13:34:25 +0000},
	Date-Modified = {2017-07-19 13:34:42 +0000},
	Howpublished = {\url{ http://bigwww.epfl.ch/sage/soft/localnormalization/}},
	Keywords = {Sigma},
	Title = {Local Normalization},
	Year = {2002 (accessed December 2013)}}

@book{saad2003iterative,
	Author = {Saad, Yousef},
	Date-Added = {2017-07-19 13:34:25 +0000},
	Date-Modified = {2017-07-19 13:34:25 +0000},
	Publisher = {SIAM},
	Title = {Iterative methods for sparse linear systems},
	Year = {2003}}

@article{okikiolu1992characterization,
	Author = {Okikiolu, Kate},
	Date-Added = {2017-07-17 17:09:31 +0000},
	Date-Modified = {2017-07-17 17:09:40 +0000},
	Journal = {Journal of the London Mathematical Society},
	Keywords = {Particle Cells;Dyadic Cubes},
	Number = {2},
	Pages = {336--348},
	Publisher = {Wiley Online Library},
	Title = {Characterization of subsets of rectifiable curves in Rn},
	Volume = {2},
	Year = {1992}}

@article{sweldens1998lifting,
	Author = {Sweldens, Wim},
	Date-Added = {2017-07-10 09:20:50 +0000},
	Date-Modified = {2017-07-10 09:20:58 +0000},
	Journal = {SIAM journal on mathematical analysis},
	Keywords = {Second Gen Wavelets},
	Number = {2},
	Pages = {511--546},
	Publisher = {SIAM},
	Title = {The lifting scheme: A construction of second generation wavelets},
	Volume = {29},
	Year = {1998}}

@article{bertoluzza1996wavelet,
	Author = {Bertoluzza, S and Naldi, G},
	Date-Added = {2017-07-10 07:54:56 +0000},
	Date-Modified = {2017-07-10 07:55:32 +0000},
	Journal = {Applied and Computational Harmonic Analysis},
	Keywords = {Wavelet PDE},
	Number = {1},
	Pages = {1--9},
	Publisher = {Elsevier},
	Title = {A wavelet collocation method for the numerical solution of partial differential equations},
	Volume = {3},
	Year = {1996}}

@article{demaret2006image,
	Annote = {Used example for figure.},
	Author = {Demaret, Laurent and Dyn, Nira and Iske, Armin},
	Date-Added = {2017-07-10 06:37:39 +0000},
	Date-Modified = {2017-07-10 06:39:28 +0000},
	Journal = {Signal Processing},
	Keywords = {Background IMesh;Figure},
	Number = {7},
	Pages = {1604--1616},
	Publisher = {Elsevier},
	Title = {Image compression by linear splines over adaptive triangulations},
	Volume = {86},
	Year = {2006}}

@article{christopoulos2000jpeg2000,
	Author = {Christopoulos, Charilaos and Skodras, Athanassios and Ebrahimi, Touradj},
	Date-Added = {2017-07-09 12:30:19 +0000},
	Date-Modified = {2017-07-09 12:30:25 +0000},
	Journal = {IEEE transactions on consumer electronics},
	Keywords = {JPEG2000},
	Number = {4},
	Pages = {1103--1127},
	Publisher = {IEEE},
	Title = {The JPEG2000 still image coding system: an overview},
	Volume = {46},
	Year = {2000}}

@article{tosic2011dictionary,
	Author = {Tosic, Ivana and Frossard, Pascal},
	Date-Added = {2017-07-09 10:49:14 +0000},
	Date-Modified = {2017-07-09 10:49:21 +0000},
	Journal = {IEEE Signal Processing Magazine},
	Keywords = {Dictionary},
	Number = {2},
	Pages = {27--38},
	Publisher = {IEEE},
	Title = {Dictionary learning},
	Volume = {28},
	Year = {2011}}

@article{plonka2009easy,
	Author = {Plonka, Gerlind},
	Date-Added = {2017-07-09 09:43:06 +0000},
	Date-Modified = {2017-07-09 09:43:15 +0000},
	Journal = {Multiscale Modeling \& Simulation},
	Keywords = {Background Wavelets;EPWT},
	Number = {3},
	Pages = {1474--1496},
	Publisher = {SIAM},
	Title = {The easy path wavelet transform: A new adaptive wavelet transform for sparse representation of two-dimensional data},
	Volume = {7},
	Year = {2009}}

@book{mallat2008wavelet,
	Author = {Mallat, Stephane},
	Date-Added = {2017-07-08 19:52:38 +0000},
	Date-Modified = {2017-07-08 19:52:44 +0000},
	Keywords = {Background Wavelets},
	Publisher = {Academic press},
	Title = {A wavelet tour of signal processing: the sparse way},
	Year = {2008}}

@article{devore1992surface,
	Author = {DeVore, Ronald A and Jawerth, Bj{\"o}rn and Lucier, Bradley J},
	Date-Added = {2017-07-08 18:29:41 +0000},
	Date-Modified = {2017-07-08 18:30:19 +0000},
	Journal = {Computer Aided Geometric Design},
	Keywords = {Background Wavelets;Reconstruction Condition;Linf},
	Number = {3},
	Pages = {219--239},
	Publisher = {Elsevier},
	Title = {Surface compression},
	Volume = {9},
	Year = {1992}}

@article{donoho2006compressed,
	Annote = { If   is known to be compressible by transform coding with a known transform, and we recon- struct via the nonlinear procedure defined here, the number of measurements   can be dramatically smaller than the size   . Thus, certain natural classes of images with   pixels need only nonadaptive nonpixel samples for faithful recovery, as opposed to the usual   pixel samples. More specifically, suppose has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)---so the coefficients belong to an     ball for . The   most important coefficients in that expansion allow reconstruction with   error . It is possible to design nonadaptive measurements allowing reconstruction with accuracy comparable to that attain- able with direct knowledge of the   most important coefficients. Moreover, a good approximation to those   important coeffi- cients is extracted from the   measurements by solving a linear program---Basis Pursuit in signal processing. The nonadaptive measurements have the character of ``random'' linear combi- nations of basis/frame elements.},
	Author = {Donoho, David L},
	Date-Added = {2017-07-08 12:17:23 +0000},
	Date-Modified = {2017-07-08 12:30:14 +0000},
	Journal = {IEEE Transactions on information theory},
	Keywords = {Background Compressed Sensing},
	Number = {4},
	Pages = {1289--1306},
	Publisher = {IEEE},
	Title = {Compressed sensing},
	Volume = {52},
	Year = {2006}}

@article{mallat1992singularity,
	Author = {Mallat, Stephane and Hwang, Wen Liang},
	Date-Added = {2017-07-08 10:53:51 +0000},
	Date-Modified = {2017-07-08 10:54:04 +0000},
	Journal = {IEEE transactions on information theory},
	Keywords = {Wavelets Reference;Background Wavelets},
	Number = {2},
	Pages = {617--643},
	Publisher = {IEEE},
	Title = {Singularity detection and processing with wavelets},
	Volume = {38},
	Year = {1992}}

@article{daubechies1988orthonormal,
	Author = {Daubechies, Ingrid},
	Date-Added = {2017-07-08 10:38:08 +0000},
	Date-Modified = {2017-07-08 10:38:17 +0000},
	Journal = {Communications on pure and applied mathematics},
	Keywords = {Background Wavelets},
	Number = {7},
	Pages = {909--996},
	Publisher = {Wiley Online Library},
	Title = {Orthonormal bases of compactly supported wavelets},
	Volume = {41},
	Year = {1988}}

@book{christensen2010functions,
	Author = {Christensen, Ole},
	Date-Added = {2017-07-08 10:35:25 +0000},
	Date-Modified = {2017-07-08 10:35:35 +0000},
	Keywords = {Wavelets Reference},
	Publisher = {Springer Science \& Business Media},
	Title = {Functions, spaces, and expansions: mathematical tools in physics and engineering},
	Year = {2010}}

@article{adelson1984pyramid,
	Author = {Adelson, Edward H and Anderson, Charles H and Bergen, James R and Burt, Peter J and Ogden, Joan M},
	Date-Added = {2017-07-07 10:21:56 +0000},
	Date-Modified = {2017-07-07 10:22:16 +0000},
	Journal = {RCA engineer},
	Keywords = {Background Tree;Pyramid},
	Number = {6},
	Pages = {33--41},
	Title = {Pyramid methods in image processing},
	Volume = {29},
	Year = {1984}}

@article{faure2016workflow,
	Author = {Faure, Emmanuel and Savy, Thierry and Rizzi, Barbara and Melani, Camilo and Sta{\v{s}}ov{\'a}, Olga and Fabr{\`e}ges, Dimitri and {\v{S}}pir, R{\'o}bert and Hammons, Mark and {\v{C}}{\'u}nderl{\'\i}k, R{\'o}bert and Recher, Ga{\"e}lle and others},
	Date-Added = {2017-07-06 17:17:06 +0000},
	Date-Modified = {2017-07-06 17:28:29 +0000},
	Journal = {Nature communications},
	Keywords = {SPIM Pipeline},
	Publisher = {Nature Publishing Group},
	Title = {A workflow to process 3D+ time microscopy images of developing organisms and reconstruct their cell lineage},
	Volume = {7},
	Year = {2016}}

@article{wait2014visualization,
	Author = {Wait, Eric and Winter, Mark and Bjornsson, Chris and Kokovay, Erzsebet and Wang, Yue and Goderie, Susan and Temple, Sally and Cohen, Andrew R},
	Date-Added = {2017-07-06 17:14:41 +0000},
	Date-Modified = {2017-07-06 17:16:07 +0000},
	Journal = {BMC bioinformatics},
	Keywords = {SPIM Pipeline;SPIM Tracking;GPU},
	Number = {1},
	Pages = {328},
	Publisher = {BioMed Central},
	Title = {Visualization and correction of automated segmentation, tracking and lineaging from 5-D stem cell image sequences},
	Volume = {15},
	Year = {2014}}

@article{oates2009quantitative,
	Annote = {The tissues of a developing embryo are simultaneously patterned, moved and differentiated according to an exchange of information between their constituent cells. We argue that these complex self-organizing phenomena can only be fully understood with quantitative mathematical frameworks that allow specific hypotheses to be formulated and tested. The quantitative and dynamic imaging of growing embryos at the molecular, cellular and tissue level is the key experimental advance required to achieve this interaction between theory and experiment. Here we describe how mathematical modelling has become an invaluable method to integrate quantitative biological information across temporal and spatial scales, serving to connect the activity of regulatory molecules with the morphological development of organisms.},
	Author = {Oates, Andrew C and Gorfinkiel, Nicole and Gonzalez-Gaitan, Marcos and Heisenberg, Carl-Philipp},
	Date-Added = {2017-07-06 16:54:26 +0000},
	Date-Modified = {2017-07-06 16:56:32 +0000},
	Journal = {Nature Reviews Genetics},
	Keywords = {STB Motivation},
	Number = {8},
	Pages = {517--530},
	Publisher = {Nature Publishing Group},
	Title = {Quantitative approaches in developmental biology},
	Volume = {10},
	Year = {2009}}

@article{sbalzarini2013modeling,
	Author = {Sbalzarini, Ivo F},
	Date-Added = {2017-07-06 16:44:43 +0000},
	Date-Modified = {2017-07-06 16:54:21 +0000},
	Journal = {Bioessays},
	Keywords = {STB Motivation;IBSB},
	Number = {5},
	Pages = {482--490},
	Publisher = {Wiley Online Library},
	Title = {Modeling and simulation of biological systems from image data},
	Volume = {35},
	Year = {2013}}

@inproceedings{zhao2002lossless,
	Annote = {The volumetric data set is important in many scientific and biomedical fields. Since such sets may be extremely large, a compression method is critical to store and transmit them. To achieve a high compression rate, most of the existing volume compression methods are lossy, which is usually unacceptable in biomedical applications. We developed a new context-based non-linear prediction method to preprocess the volume data set in order to effectively lower the prediction entropy. The prediction error is further encoded using Huffman code. Unlike the conventional methods, the volume is divided into cubical blocks to take advantage of the data's spatial locality. Instead of building one Huffman tree for each block, we developed a novel binning algorithm that build a Huffman tree for each group (bin) of blocks. Combining all the effects above, we achieved an excellent compression rate compared to other lossless volume compression methods. In addition, an auxiliary data structure, Scalable Hyperspace File (SHSF) is used to index the huge volume so that we can obtain many other benefits including parallel construction, on-the-fly accessing of compressed data without global decompression, fast previewing, efficient background compressing, and scalability etc.},
	Author = {Zhao, Rongkai and Tao, Tao and Gabriel, Michael and Belford, Geneva G},
	Booktitle = {Proc. SPIE},
	Date-Added = {2017-07-06 07:07:54 +0000},
	Date-Modified = {2017-07-06 07:10:08 +0000},
	Keywords = {Background Tree;SPT},
	Pages = {180},
	Title = {Lossless compression of very large volume data with fast dynamic access},
	Volume = {4925},
	Year = {2002}}

@article{liang2017namlet,
	Author = {Liang, Hu and Zhao, Shengrong and Chen, Chuanbo and Sarem, Mudar},
	Date-Added = {2017-07-06 06:52:15 +0000},
	Date-Modified = {2017-07-06 06:52:58 +0000},
	Journal = {Signal Processing},
	Keywords = {Background Wavelets;Recent},
	Pages = {251--263},
	Publisher = {Elsevier},
	Title = {The NAMlet transform: A novel image sparse representation method based on non-symmetry and anti-packing model},
	Volume = {137},
	Year = {2017}}

@article{radha1991binary,
	Author = {Radha, Hayder and Leonardi, Riccardo and Vetterli, Martin and Naylor, Bruce},
	Date-Added = {2017-07-06 06:50:44 +0000},
	Date-Modified = {2017-07-06 06:51:08 +0000},
	Journal = {Journal of Visual Communication and Image Representation},
	Keywords = {Background Tree},
	Number = {3},
	Pages = {201--221},
	Publisher = {Elsevier},
	Title = {Binary space partitioning tree representation of images},
	Volume = {2},
	Year = {1991}}

@article{wang1994active,
	Annote = {his paper introduces a representation scheme for
image sequences using nonuniform samples embedded in a deformable
mesh structure. It describes a sequence by nodal positions
and colors in a starting frame, followed by nodal displacements
in the following frames. The nodal points in the mesh
are more densely distributed in regions containing interesting
features such as edges and corners; and are dynamically updated
to follow the same features in successive frames. They are determined automatically by maximizing feature (e.g, gradient)
magnitudes at nodal points, while minimizing interpolation errors
within individual elements, and matching errors between
corresponding elements. In order to avoid the mesh elements
becoming overly deformed, a penalty term is also incorporated,
which measures the irregularity of the mesh structure. The
notions of shape functions and master elements commonly used
in the finite element method have been applied to simplify the
numerical calculation of the energy functions and their gradients.
The proposed representation is motivated by the active contour
or snake model proposed by Kass, Witkin, and Terzopoulos. The
current representation retains the salient merit of the original
model as a feature tracker based on local and collective information,
while facilitating more accurate image interpolation
and prediction. Our computer simulations have shown that the
proposed scheme can successfully track facial feature movements
in head-and-shoulder type of sequences, and more generally,
interframe changes that can be modeled as elastic deformation.

Ideas from FEM

--->The treatment for the starting frame also constitutes an efficient
representation of arbitrary still images. <---

TRacking of features through time},
	Author = {Wang, Yao and Lee, Ouseb},
	Date-Added = {2017-07-06 06:46:59 +0000},
	Date-Modified = {2017-07-06 06:48:11 +0000},
	Journal = {IEEE Transactions on image processing},
	Keywords = {Background IMesh},
	Number = {5},
	Pages = {610--624},
	Publisher = {IEEE},
	Title = {Active mesh-a feature seeking and tracking image sequence representation scheme},
	Volume = {3},
	Year = {1994}}

@inproceedings{jansen2001multiscale,
	Annote = {Multiresolution triangulation meshes are widely used in computer
graphics for 3-d modeling of shapes. We propose an image representation
and processing framework using a multiscale triangulation
of the grayscale function. Triangles have the potential of
approximating edges better than the blocky structures of tensorproduct
wavelets. Among the many possible triangulation schemes,
normal meshes are natural for efficiently representing singularities
in image data thanks to their adaptivi@ to the smoothness of the
modeled image. Our non-linear, multiscale image decomposition
algorithm, based on this subdivision scheme, takes edges into account
in a way that is closely related to wedgelets and curvelets.
The highly adaptive property of the normal mesh construction provides
a very efficient representation of images, which potentially
outperforms standard wavelet transforms. We demonstrate the approximation
performance of the normal mesh representation through
mathematical analyses for simple functions and simulations for
real images. },
	Author = {Jansen, Maarten and Choi, Hyeokho and Lavu, Sridhar and Baraniuk, Richard},
	Booktitle = {Image Processing, 2001. Proceedings. 2001 International Conference on},
	Date-Added = {2017-07-06 06:44:31 +0000},
	Date-Modified = {2017-07-06 06:45:40 +0000},
	Keywords = {Background IMesh},
	Organization = {IEEE},
	Pages = {229--232},
	Title = {Multiscale image processing using normal triangulated meshes},
	Volume = {2},
	Year = {2001}}

@inproceedings{iske2015optimally,
	Author = {Iske, Armin and Demaret, Laurent},
	Booktitle = {Sampling Theory and Applications (SampTA), 2015 International Conference on},
	Date-Added = {2017-07-06 06:41:00 +0000},
	Date-Modified = {2017-07-06 06:41:30 +0000},
	Keywords = {Background Content;Background IMesh;Optimality},
	Organization = {IEEE},
	Pages = {463--467},
	Title = {Optimally sparse image approximation by adaptive linear splines over anisotropic triangulations},
	Year = {2015}}

@inproceedings{lee1998maps,
	Annote = {We construct smooth parameterizations of irregular connectivity tri- angulations of arbitrary genus 2-manifolds. Our algorithm uses hi- erarchical simplification to efficiently induce a parameterization of the original mesh over a base domain consisting of a small num- ber of triangles. This initial parameterization is further improved through a hierarchical smoothing procedure based on Loop sub- division applied in the parameter domain. Our method supports both fully automatic and user constrained operations. In the lat- ter, we accommodate point and edge constraints to force the align-ment of iso-parameter lines with desired features. We show how to use the parameterization for fast, hierarchical subdivision con- nectivity remeshing with guaranteed error bounds. The remeshing algorithm constructs an adaptively subdivided mesh directly with- out first resorting to uniform subdivision followed by subsequent sparsification. It thus avoids the exponential cost of the latter. Our parameterizations are also useful for texture mapping and morphing applications, among others.

 We describe an O(N log N ) time and storage algorithm to con- struct a logarithmic level hierarchy of arbitrary topology, ir- regular connectivity meshes based on the Dobkin-Kirkpatrick (DK) algorithm. Our algorithm accommodates geometric crite- ria such as area and curvature as well as vertex and edge con- straints.
* We construct a smooth parameterization of the original mesh over the base domain. This parameterization is derived through repeated conformal remapping during graph simplification fol- lowed by a parameter space smoothing procedure based on the Loop scheme. The resulting parameterizations are of high visual and numerical quality.
* Using the smooth parameterization, we describe an algorithm for adaptive, hierarchical remeshing of arbitrary meshes into subdivision connectivity meshes. The procedure is fully auto- matic, but also allows for user intervention in the form of fixing point or path features in the original mesh. The remeshed man- ifold meets conservative approximation bounds.},
	Author = {Lee, Aaron WF and Sweldens, Wim and Schr{\"o}der, Peter and Cowsar, Lawrence and Dobkin, David},
	Booktitle = {Proceedings of the 25th annual conference on Computer graphics and interactive techniques},
	Date-Added = {2017-07-06 06:34:19 +0000},
	Date-Modified = {2017-07-06 06:35:41 +0000},
	Keywords = {Background Computer Graphics;Background IMesh},
	Organization = {ACM},
	Pages = {95--104},
	Title = {MAPS: Multiresolution adaptive parameterization of surfaces},
	Year = {1998}}

@article{henderson2003human,
	Author = {Henderson, John M},
	Date-Added = {2017-07-03 17:56:30 +0000},
	Date-Modified = {2017-07-03 17:56:48 +0000},
	Journal = {Trends in cognitive sciences},
	Keywords = {VisualSystem},
	Number = {11},
	Pages = {498--504},
	Publisher = {Elsevier},
	Title = {Human gaze control during real-world scene perception},
	Volume = {7},
	Year = {2003}}

@article{peng2010v3d,
	Author = {Peng, Hanchuan and Ruan, Zongcai and Long, Fuhui and Simpson, Julie H and Myers, Eugene W},
	Date-Added = {2017-07-02 18:24:30 +0000},
	Date-Modified = {2017-07-02 18:25:09 +0000},
	Journal = {Nature biotechnology},
	Keywords = {SPIM Visualization;Background RealTime},
	Number = {4},
	Pages = {348--353},
	Publisher = {Nature Research},
	Title = {V3D enables real-time 3D visualization and quantitative analysis of large-scale biological image data sets},
	Volume = {28},
	Year = {2010}}

@article{rafael2002digital,
	Annote = {General Image Procesing Text},
	Author = {Rafael Gonzalez, C and Woods, Richard},
	Date-Added = {2017-06-30 12:31:18 +0000},
	Date-Modified = {2017-06-30 12:31:31 +0000},
	Journal = {Pearson Education},
	Keywords = {ImageProcessing},
	Title = {Digital image processing},
	Year = {2002}}

@article{jinek2012programmable,
	Author = {Jinek, Martin and Chylinski, Krzysztof and Fonfara, Ines and Hauer, Michael and Doudna, Jennifer A and Charpentier, Emmanuelle},
	Date-Added = {2017-06-28 17:28:28 +0000},
	Date-Modified = {2017-06-28 17:28:53 +0000},
	Journal = {Science},
	Keywords = {Microscopy Background;CRISPR},
	Number = {6096},
	Pages = {816--821},
	Publisher = {American Association for the Advancement of Science},
	Title = {A programmable dual-RNA--guided DNA endonuclease in adaptive bacterial immunity},
	Volume = {337},
	Year = {2012}}

@article{prasher1992primary,
	Author = {Prasher, Douglas C and Eckenrode, Virginia K and Ward, William W and Prendergast, Frank G and Cormier, Milton J},
	Date-Added = {2017-06-28 17:22:31 +0000},
	Date-Modified = {2017-06-28 17:23:00 +0000},
	Journal = {Gene},
	Keywords = {Background Microscope;GFP},
	Number = {2},
	Pages = {229--233},
	Publisher = {Elsevier},
	Title = {Primary structure of the Aequorea victoria green-fluorescent protein},
	Volume = {111},
	Year = {1992}}

@article{cheeseman2014cell,
	Author = {Cheeseman, Bevan L and Zhang, Dongcheng and Binder, Benjamin J and Newgreen, Donald F and Landman, Kerry A},
	Date-Added = {2017-06-28 15:48:56 +0000},
	Date-Modified = {2017-06-28 15:49:11 +0000},
	Journal = {Journal of The Royal Society Interface},
	Keywords = {Background Spatio-temporal},
	Number = {93},
	Pages = {20130815},
	Publisher = {The Royal Society},
	Title = {Cell lineage tracing in the developing enteric nervous system: superstars revealed by experiment and simulation},
	Volume = {11},
	Year = {2014}}

@article{van1998image,
	Author = {Van Vliet, Lucas J and Boddeke, Frank R and Sudar, Damir and Young, Ian T},
	Date-Added = {2017-06-28 10:35:02 +0000},
	Date-Modified = {2017-06-28 10:35:09 +0000},
	Journal = {Digital Image Analysis of Microbes: Imaging, Morphometry, Fluorometry, and Motility Techniques and Applications},
	Keywords = {Microscopy Background},
	Pages = {37--63},
	Title = {Image detectors for digital image microscopy},
	Year = {1998}}

@inproceedings{preibisch2009bead,
	Author = {Preibisch, Stephan and Saalfeld, Stephan and Rohlfing, Torsten and Tomancak, Pavel},
	Booktitle = {Proc. of SPIE Vol},
	Date-Added = {2017-06-27 16:14:15 +0000},
	Date-Modified = {2017-06-27 16:14:31 +0000},
	Keywords = {SPIM Fusion;Background SPIM Processing},
	Pages = {72592S--1},
	Title = {Bead-based mosaicing of single plane illumination microscopy images using geometric local descriptor matching},
	Volume = {7259},
	Year = {2009}}

@article{escande2015sparse,
	Author = {Escande, Paul and Weiss, Pierre},
	Date-Added = {2017-06-27 15:38:30 +0000},
	Date-Modified = {2017-07-02 13:10:07 +0000},
	Journal = {SIAM Journal on Imaging Sciences},
	Keywords = {SPIM Deconvolution;IntensityScale},
	Number = {4},
	Pages = {2976--3014},
	Publisher = {SIAM},
	Title = {Sparse wavelet representations of spatially varying blurring operators},
	Volume = {8},
	Year = {2015}}

@phdthesis{huisken2004multi,
	Annote = {Specifically in biology and medicine, optical microscopy tech- niques are well established and widespread. They provide a noninvasive and nondestructive way of imaging processes in live specimens.

The technique of fluorescence microscopy is one of the main applications of microscopy in biological and medical research. The sample can be fluorescently labelled with a variety of dyes that specifically bind to organelles, compartments of the cell or single proteins. Nowadays, the family of green fluorescent proteins (GFP) with its spectral variants are routinely applied to study gene and protein expression and to track single molecules in whole animals and single cells in vivo. The focus of this work is mainly on fluorescence microscopy.},
	Author = {Huisken, Jan},
	Date-Added = {2017-06-27 15:13:15 +0000},
	Date-Modified = {2017-06-27 15:16:48 +0000},
	Keywords = {Microscopy Background},
	Title = {Multi-view Microscopy and Multi-beam Manipulation for High-resolution Optical Imaging},
	Year = {2004}}

@incollection{vicidomini2005image,
	Author = {Vicidomini, Giuseppe},
	Booktitle = {From Cells to Proteins: Imaging Nature across Dimensions},
	Date-Added = {2017-06-27 14:52:31 +0000},
	Date-Modified = {2017-06-27 14:52:43 +0000},
	Keywords = {Microscopy Background},
	Pages = {371--393},
	Publisher = {Springer},
	Title = {Image Formation in Fluorescence Microscopy},
	Year = {2005}}

@article{reinagel1999natural,
	Annote = { ``Similarly, human eye movements influence the statistics of the effective visual input. Humans move their eyes several times a second when looking at a scene. The portions of a scene that fall on the fovea are sampled at high spatial resolution, and receive a disproportionate fraction of cortical processing.''

``We conclude that eye movements change visual input, not only for complex features like faces, but also for simple local properties like contrast and spatial correlation. Thus eye movements might serve to increase the information content of the visual input at multiple levels.''

Ideas of the local adaptation of focus. Further, on the focus being at higher scale such as faces, but at lower level, in response to the scene structure.

},
	Author = {Reinagel, Pamela and Zador, Anthony M},
	Date-Added = {2017-06-26 14:40:11 +0000},
	Date-Modified = {2017-06-27 14:21:51 +0000},
	Journal = {Network: Computation in Neural Systems},
	Keywords = {VisualSystem;Adapt},
	Number = {4},
	Pages = {341--350},
	Publisher = {Taylor \& Francis},
	Title = {Natural scene statistics at the centre of gaze},
	Volume = {10},
	Year = {1999}}

@article{koch2006much,
	Annote = {With approximately 10^6 ganglion cells, the human retina would transmit data at roughly the rate of an Ethernet connection.

10^5 ganglion cells transmit on the order of 875,000 bits per second
 
8,750,000 bits per second},
	Author = {Koch, Kristin and McLean, Judith and Segev, Ronen and Freed, Michael A and Berry, Michael J and Balasubramanian, Vijay and Sterling, Peter},
	Date-Added = {2017-06-26 14:30:48 +0000},
	Date-Modified = {2017-06-27 13:48:00 +0000},
	Journal = {Current Biology},
	Keywords = {VisualSystem},
	Number = {14},
	Pages = {1428--1434},
	Publisher = {Elsevier},
	Title = {How much the eye tells the brain},
	Volume = {16},
	Year = {2006}}

@article{brenner2000adaptive,
	Annote = {``Adaptation to mean light level ensures that our visual responses are matched to the average signal in real time, thus maintaining sensitivity to the fluctuations around this mean. But the fluctuations themselves are intermittent, such that periods of large fluctuations are interspersed with periods of relative ``quiet.'' Recent ob- servations indicate that the intermittency of natural sig- nals is of a special form: statistics such as the variance and correlation function are stationary over some re- gions, with slowly modulated parameters over larger regions (Ruderman and Bialek, 1994; Nelken et al., 1999). The principle of efficient coding suggests that the ner- vous system should adapt its strategy to these local statistical properties of the stimulus. Evidence for such statistical adaptation in the early stages of vision has been found in the fly (van Hateren, 1997) and in the vertebrate retina (Smirnakis et al., 1997), where mecha- nisms of gain control are well known (Shapley and Victor, 1979a, 1979b, 1980).''

 Previous work has shown that this system adapts to constant velocity signals (Maddess and Laugh- lin, 1985; de Ruyter van Steveninck et al., 1986), to the variance of spatial image contrast (de Ruyter van Ste- veninck et al., 1996), 

We find a dramatic adaptive rescaling of the system's input/output relation with the standard deviation of the signal distribution. Further, we find that the magnitude of the rescaling selected by the adaptation process opti- mizes information transmission.},
	Author = {Brenner, Naama and Bialek, William and Van Steveninck, Rob de Ruyter},
	Date-Added = {2017-06-26 14:26:18 +0000},
	Date-Modified = {2017-06-27 14:21:30 +0000},
	Journal = {Neuron},
	Keywords = {VisualSystem;Sigma},
	Number = {3},
	Pages = {695--702},
	Publisher = {Elsevier},
	Title = {Adaptive rescaling maximizes information transmission},
	Volume = {26},
	Year = {2000}}

@inproceedings{ruderman1994statistics,
	Annote = {Shows that when normalized by a standard deviation of the gradients follows a rayliegh distribution consistent then with the gradients by normalized, then maximising entropy.

Suggest a link between this local variance, efficient represent through the use of a local contrast control.

``Variance normal- ization seems crucial to an eEcient representation of im- age data, and there may be a connection between vari- ance normalization as defined here and the various types of contrast gain control observed throughout the visual pathways [12].''},
	Author = {Ruderman, Daniel L and Bialek, William},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2017-06-26 14:24:17 +0000},
	Date-Modified = {2017-06-27 13:53:11 +0000},
	Keywords = {VisualSystem;Sigma},
	Pages = {551--558},
	Title = {Statistics of natural images: Scaling in the woods},
	Year = {1994}}

@article{smirnakis1997adaptation,
	Annote = {``Ambient light levels varies daily over more than nine orders of magnitude, where as the firing rate of an optic nerve fires less than two.''

This is alleviated through adaptation. This is linked to the range of light intensities (gain control) and also the spatial scales in the scene. },
	Author = {Smirnakis, Stelios M and Berry, Michael J and Warland, David K and Bialek, William and Meister, Markus},
	Date-Added = {2017-06-26 14:10:42 +0000},
	Date-Modified = {2017-06-27 14:22:02 +0000},
	Journal = {Nature},
	Keywords = {VisualSystem;Sigma;Adapt},
	Number = {6620},
	Pages = {69},
	Publisher = {Nature Publishing Group},
	Title = {Adaptation of retinal processing to image contrast and spatial scale},
	Volume = {386},
	Year = {1997}}

@article{strobl2017improving,
	Annote = {Although LSFM benefits from the development of larger and faster cameras, data processing remains a limiting factor and provides ample opportunities for the development of sophisticated software. 

In general, novel technologies should provide opportunities to conceive and perform experiments that could not have been imagined before. LSFM has already raised fluorescence microscopy to a new level, but it has by no means reached
its full potential yet. The next decade will surely surprise us with sophisticated technological advancements and exciting applications in many different biological disciplines.},
	Author = {Strobl, Frederic and Schmitz, Alexander and Stelzer, Ernst HK},
	Date-Added = {2017-06-26 13:14:06 +0000},
	Date-Modified = {2017-07-02 12:42:44 +0000},
	Journal = {Nature Protocols},
	Keywords = {Background SPIM;Background SPIM Processing;Background DataProblem},
	Number = {6},
	Pages = {1103--1109},
	Publisher = {Nature Research},
	Title = {Improving your four-dimensional image: traveling through a decade of light-sheet-based fluorescence microscopy research},
	Volume = {12},
	Year = {2017}}

@article{schindelin2012fiji,
	Author = {Schindelin, Johannes and Arganda-Carreras, Ignacio and Frise, Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan and Schmid, Benjamin and others},
	Date-Added = {2017-06-26 13:08:24 +0000},
	Date-Modified = {2017-06-26 13:08:39 +0000},
	Journal = {Nature methods},
	Keywords = {Background SPIM Processing;SPIM Pipeline},
	Number = {7},
	Pages = {676--682},
	Publisher = {Nature Research},
	Title = {Fiji: an open-source platform for biological-image analysis},
	Volume = {9},
	Year = {2012}}

@article{wolff2017reconstruction,
	Annote = { Cell tracking with a new tool called Massive Multi-view Tracker
(MaMuT) enabled the reconstruction of the complete cell lineage of an outgrowing
thoracic limb with single-cell resolution.},
	Author = {Wolff, Carsten and Tinevez, Jean-Yves and Pietzsch, Tobias and Stamataki, Evangelia and Harich, Benjamin and Preibisch, Stephan and Shorte, Spencer and Keller, Philipp J and Tomancak, Pavel and Pavlopoulos, Anastasios},
	Date-Added = {2017-06-26 13:03:13 +0000},
	Date-Modified = {2017-06-26 13:06:53 +0000},
	Journal = {bioRxiv},
	Keywords = {Background SPIM Processing;SPIM Tracking},
	Pages = {112623},
	Publisher = {Cold Spring Harbor Labs Journals},
	Title = {Reconstruction of cell lineages and behaviors underlying arthropod limb outgrowth with multi-view light-sheet imaging and tracking},
	Year = {2017}}

@inproceedings{jensen2016active,
	Annote = {To fully utilize the potential of the modality requires a segmen- tation of anatomical regions which is particularly challenging due to image distortions affecting both intensities and shape anatomy. In animal studies the anatomic distortions may be caused by brain ex- traction and chemical tissue clearing, which are performed prior to scanning and allows the light sheet to penetrate with minimal scat- tering and absorption [2]. The tissue clearing procedure also intro- duces unwanted intensity distortions since the efficiency varies be- tween subjects. The optical elements inside the LSFM system also causes intensity distortions which may appear differently across sub- jects but generally appear as slowly varying spatial inhomogeneities of the intensities.
These distortions pose a challenge to image segmentation meth- ods. The most common form of LSFM-related segmentation con- cerns the identification of highly local intensity build ups [3], used to investigate cellular morphologies within anatomical regions of in- terest.},
	Author = {Jensen, Casper Bo and Lyksborg, Mark and Hecksher-S, J and Secher, Anna and Conradsen, Knut and Dahl, Anders Bjorholm and others},
	Booktitle = {Biomedical Imaging (ISBI), 2016 IEEE 13th International Symposium on},
	Date-Added = {2017-06-26 13:00:05 +0000},
	Date-Modified = {2017-07-02 13:33:00 +0000},
	Keywords = {Background SPIM Processing;SPIM Segmentation;AnatomicalRegions;IntensityScale},
	Organization = {IEEE},
	Pages = {217--220},
	Title = {Active appearance segmentation for intensity inhomogeneity in light sheet fluorescence microscopy},
	Year = {2016}}

@article{mathew2015robust,
	Abstract = {Due to the large amount of data produced by advanced microscopy, automated image analysis is crucial in modern biology. Most applications require reliable cell nuclei segmentation. However, in many biological specimens cell nuclei are densely packed and appear to touch one another in the images. Therefore, a major difficulty of three-dimensional cell nuclei segmentation is the decomposition of cell nuclei that apparently touch each other. Current methods are highly adapted to a certain biological specimen or a specific microscope. They do not ensure similarly accurate segmentation performance, i.e. their robustness for different datasets is not guaranteed. Hence, these methods require elaborate adjustments to each dataset.},
	Author = {Mathew, B. and Schmitz, A. and Mu{\~{n}}oz-Descalzo, S. and Ansari, N. and Pampaloni, F. and Stelzer, E.H.K. and Fischer, S.C.},
	Date-Added = {2017-06-26 12:55:20 +0000},
	Date-Modified = {2017-07-02 13:34:20 +0000},
	Doi = {10.1186/s12859-015-0617-x},
	Issn = {1471-2105},
	Journal = {BMC Bioinformatics},
	Keywords = {Background SPIM Processing;SPIM Segmentation;Nuclei},
	Number = {1},
	Pages = {187},
	Title = {Robust and automated three-dimensional segmentation of densely packed cell nuclei in different biological specimens with Lines-of-Sight decomposition},
	Url = {http://dx.doi.org/10.1186/s12859-015-0617-x},
	Volume = {16},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1186/s12859-015-0617-x}}

@article{lou2014rapid,
	Author = {Lou, Xinghua and Kang, Minjung and Xenopoulos, Panagiotis and Munoz-Descalzo, Silvia and Hadjantonakis, Anna-Katerina},
	Date-Added = {2017-06-26 12:46:54 +0000},
	Date-Modified = {2017-07-02 13:25:31 +0000},
	Journal = {Stem cell reports},
	Keywords = {SPIM Segmentation;Nuclei},
	Number = {3},
	Pages = {382--397},
	Publisher = {Elsevier},
	Title = {A rapid and efficient 2D/3D nuclear segmentation method for analysis of early mouse embryo and stem cell image data},
	Volume = {2},
	Year = {2014}}

@article{gole2016opensegspim,
	Author = {Gole, Laurent and Ong, Kok Haur and Boudier, Thomas and Yu, Weimiao and Ahmed, Sohail},
	Date-Added = {2017-06-26 12:43:35 +0000},
	Date-Modified = {2017-06-26 12:43:54 +0000},
	Journal = {Bioinformatics},
	Keywords = {Background SPIM Processing;SPIM Segmentation},
	Number = {13},
	Pages = {2075--2077},
	Publisher = {Oxford University Press},
	Title = {OpenSegSPIM: a user-friendly segmentation tool for SPIM data},
	Volume = {32},
	Year = {2016}}

@article{afshar2016parallel,
	Author = {Afshar, Yaser and Sbalzarini, Ivo F},
	Date-Added = {2017-06-26 12:40:43 +0000},
	Date-Modified = {2017-07-02 13:18:20 +0000},
	Journal = {PloS one},
	Keywords = {Background SPIM Processing;SPIM Segmentation;CPU},
	Number = {4},
	Pages = {e0152528},
	Publisher = {Public Library of Science},
	Title = {A parallel distributed-memory particle method enables acquisition-rate segmentation of large fluorescence microscopy images},
	Volume = {11},
	Year = {2016}}

@article{peng2014extensible,
	Author = {Peng, Hanchuan and Bria, Alessandro and Zhou, Zhi and Iannello, Giulio and Long, Fuhui},
	Date-Added = {2017-06-26 10:07:42 +0000},
	Date-Modified = {2017-07-02 18:24:09 +0000},
	Journal = {Nature protocols},
	Keywords = {Background SPIM Processing},
	Number = {1},
	Pages = {193--208},
	Publisher = {Nature Research},
	Title = {Extensible visualization and analysis for multidimensional images using Vaa3D},
	Volume = {9},
	Year = {2014}}

@inproceedings{jiang2016automatic,
	Author = {Jiang, Hao and Yu, Tingting and Nie, Jun and Fang, Chunyu and Zhu, Dan and Fei, Peng},
	Booktitle = {Asia Communications and Photonics Conference},
	Date-Added = {2017-06-26 10:02:28 +0000},
	Date-Modified = {2017-06-26 10:02:52 +0000},
	Keywords = {SPIM Visualization;Background SPIM Processing},
	Organization = {Optical Society of America},
	Pages = {AF4K--4},
	Title = {Automatic Light-sheet Imaging Plugin for Rapid Threedimensional Visualization of Embryo Angiopoiesis on Common Wide-field Microscope},
	Year = {2016}}

@article{hunter2015problems,
	Author = {Hunter, Philip},
	Date-Added = {2017-06-26 10:00:03 +0000},
	Date-Modified = {2017-06-26 10:00:27 +0000},
	Doi = {10.15252/embr.201541061},
	Eprint = {http://embor.embopress.org/content/16/9/1068.full.pdf},
	Issn = {1469-221X},
	Journal = {EMBO reports},
	Keywords = {Background DataProblem},
	Number = {9},
	Pages = {1068--1070},
	Publisher = {EMBO Press},
	Title = {Better tools, new problems},
	Url = {http://embor.embopress.org/content/16/9/1068},
	Volume = {16},
	Year = {2015},
	Bdsk-Url-1 = {http://embor.embopress.org/content/16/9/1068},
	Bdsk-Url-2 = {http://dx.doi.org/10.15252/embr.201541061}}

@article{pietzsch2015bigdataviewer,
	Annote = {We achieve this performance on very large data by using an efficient client-side renderer and an intelligent loading and cach- ing scheme. To render any virtual slice, only a small fraction of the image data is relevant and gets loaded into memory. The navigation is further accelerated by caching recently visited locations in memory. If images are available at multiple scales, only the most relevant scales for display are requested. This avoids aliasing artifacts at zoomed- out views and facilitates interactive browsing: low-resolution data are loaded rapidly, providing immediate feedback, whereas high- resolution detail is filled in subsequently (Supplementary Note 2).
For large time series, we developed a custom open-source data format that is optimized for fast random data access at various scales (Supplementary Note 3). Each image is stored as a chunked multidimensional array at successively reduced resolutions. We build on HDF5 as an established portable data format that pro- vides efficient input and output, supports unlimited file sizes and has built-in and extensible compression facilities. Metadata, such as SPIM view registrations, are stored as XML. The proposed format integrates seamlessly with Fiji's plug-ins for SPIM image process- ing, allowing control and visualization of the intermediate steps of the pipeline (Fig. 1b--e and Supplementary Video 2). We provide Fiji plug-ins for reading and writing the format so that any image that opens in Fiji can be stored as HDF5 and viewed with the BDV plug-in (Supplementary Note 4).

The virtualized access is a powerful way to present remote and/ or extremely large data sets for computation as well as visualization. In Fiji, we use our framework to make raw images of the data sets available as virtual stacks and then run standard image processing tools on arbitrarily large images. For example, a SPIM time-lapse image series may be registered, fused and deconvolved without being locally stored on the processing computer. Moreover, it is straightforward to programmatically access the pixel data using standard ImgLib2 interfaces, which means that existing code for filtering and segmentation will work without
modification (Fig. 1j).},
	Author = {Pietzsch, Tobias and Saalfeld, Stephan and Preibisch, Stephan and Tomancak, Pavel},
	Date-Added = {2017-06-26 09:45:41 +0000},
	Date-Modified = {2017-07-02 18:34:13 +0000},
	Journal = {Nature methods},
	Keywords = {Background SPIM Processing;SPIM Visualization},
	Number = {6},
	Pages = {481--483},
	Publisher = {Nature Research},
	Title = {BigDataViewer: visualization and processing for large image data sets},
	Volume = {12},
	Year = {2015}}

@article{royer2015clearvolume,
	Author = {Royer, Loic A and Weigert, Martin and G{\"u}nther, Ulrik and Maghelli, Nicola and Jug, Florian and Sbalzarini, Ivo F and Myers, Eugene W},
	Date-Added = {2017-06-26 09:43:58 +0000},
	Date-Modified = {2017-06-26 09:44:14 +0000},
	Journal = {Nature methods},
	Keywords = {SPIM Visualization;Background SPIM Processing},
	Number = {6},
	Pages = {480--481},
	Publisher = {Nature Research},
	Title = {ClearVolume: open-source live 3D visualization for light-sheet microscopy},
	Volume = {12},
	Year = {2015}}

@article{royer2016adaptive,
	Author = {Royer, Lo{\"\i}c A and Lemon, William C and Chhetri, Raghav K and Wan, Yinan and Coleman, Michael and Myers, Eugene W and Keller, Philipp J},
	Date-Added = {2017-06-26 09:28:51 +0000},
	Date-Modified = {2017-06-26 09:29:21 +0000},
	Journal = {Nature biotechnology},
	Keywords = {Background SPIM Processing;Background RealTime;SPIM SmartM},
	Number = {12},
	Pages = {1267--1278},
	Publisher = {Nature Research},
	Title = {Adaptive light-sheet microscopy for long-term, high-resolution imaging in living organisms},
	Volume = {34},
	Year = {2016}}

@article{richardson1972bayesian,
	Author = {Richardson, William Hadley},
	Date-Added = {2017-06-26 09:25:18 +0000},
	Date-Modified = {2017-06-26 09:25:24 +0000},
	Journal = {JOSA},
	Keywords = {SPIM Deconvolution},
	Number = {1},
	Pages = {55--59},
	Publisher = {Optical Society of America},
	Title = {Bayesian-based iterative method of image restoration},
	Volume = {62},
	Year = {1972}}

@article{weigert2017isotropic,
	Annote = {t. Fluorescence microscopy images usually show severe aniso- tropy in axial versus lateral resolution. This hampers downstream pro- cessing, i.e. the automatic extraction of quantitative biological data. While deconvolution methods and other techniques to address this prob- lem exist, they are either time consuming to apply or limited in their ability to remove anisotropy. We propose a method to recover isotropic resolution from readily acquired anisotropic data. We achieve this us- ing a convolutional neural network that is trained end-to-end from the same anisotropic body of data we later apply the network to. The net- work effectively learns to restore the full isotropic resolution by restoring the image under a trained, sample specific image prior. We apply our method to 3 synthetic and 3 real datasets and show that our results im- prove on results from deconvolution and state-of-the-art superresolution techniques. Finally, we demonstrate that a standard 3D segmentation pipeline performs on the output of our network with comparable accu- racy as on the full isotropic data.},
	Author = {Weigert, Martin and Royer, Loic and Jug, Florian and Myers, Gene},
	Date-Added = {2017-06-26 09:20:14 +0000},
	Date-Modified = {2017-07-02 13:16:26 +0000},
	Journal = {arXiv preprint arXiv:1704.01510},
	Keywords = {Background SPIM Processing;SPIM Deconvolution;GPU;IntensityScale},
	Title = {Isotropic reconstruction of 3D fluorescence microscopy images using convolutional neural networks},
	Year = {2017}}

@article{schmid2015real,
	Annote = {. The measured lateral standard
deviation of the PSF was typically between 1.5 and 1.8 pixels on our
microscopes.

Graphics card
Quadro K2000 Tesla C2075 GeForce GTX 680 Tesla K40c GeForce Titan black
5123 pixel 12.0
6.6 7.8 3.9 4.0
10243 pixel 83.7
48.2 29.8 19.4 21.1
20483 pixel 683.8
378.7 238.5 153.6 152.3

For 1024^3 time is 21.1 seconds on Titan Black (Best time)
},
	Author = {Schmid, Benjamin and Huisken, Jan},
	Date-Added = {2017-06-26 09:04:22 +0000},
	Date-Modified = {2017-07-02 13:09:55 +0000},
	Journal = {Bioinformatics},
	Keywords = {Background SPIM Processing;SPIM Deconvolution;Background RealTime;SPIM PSF;GPU},
	Number = {20},
	Pages = {3398--3400},
	Publisher = {Oxford University Press},
	Title = {Real-time multi-view deconvolution},
	Volume = {31},
	Year = {2015}}

@article{schmied2015automated,
	Author = {Schmied, Christopher and Steinbach, Peter and Pietzsch, Tobias and Preibisch, Stephan and Tomancak, Pavel},
	Date-Added = {2017-06-26 09:00:00 +0000},
	Date-Modified = {2017-06-26 09:00:21 +0000},
	Journal = {Bioinformatics},
	Keywords = {Background SPIM Processing;SPIM Pipeline},
	Number = {7},
	Pages = {1112--1114},
	Publisher = {Oxford University Press},
	Title = {An automated workflow for parallel processing of large multiview SPIM recordings},
	Volume = {32},
	Year = {2015}}

@inproceedings{temerinac2011spatially,
	Author = {Temerinac-Ott, Maja and Ronneberger, Olaf and Nitschke, Roland and Driever, Wolfgang and Burkhardt, Hans},
	Booktitle = {Biomedical Imaging: From Nano to Macro, 2011 IEEE International Symposium on},
	Date-Added = {2017-06-26 08:58:01 +0000},
	Date-Modified = {2017-07-02 13:10:16 +0000},
	Keywords = {Background SPIM Processing;SPIM Deconvolution;IntensityScale},
	Organization = {IEEE},
	Pages = {899--904},
	Title = {Spatially-variant Lucy-Richardson deconvolution for multiview fusion of microscopical 3D images},
	Year = {2011}}

@inproceedings{cao2015real,
	Annote = {Light sheet fluorescence microscopy (LSFM) led researchers to get optical sections of large samples, virtually without toxicity and light bleaching and with high temporal resolution, and to record the development of large, living samples with exceptionally high information content. And images observed by LSFM with high signal to noise ratio are very suited for three-dimensional reconstruction. Deconvolution reduces blurring from out-of-focus light to improve the contrast and sharpness of image, but commercial deconvolution software is slow and expensive which cannot meet the current demand. GPU is the new many-core processor with powerful floating point performance, so we parallelized the Richardson Lucy Deconvolution on the GPU. Under ensuring image quality, the implementation on the GPU runs ~30 times faster than the implementation on the CPU. For an image of size 1024 × 1024 × 25, the deconvolved time of 50 iterations on the GPU is no more than 2 s.},
	Author = {Cao, Lianyu and Juan, Penghui and Zhang, Yinghua},
	Booktitle = {International Conference on Algorithms and Architectures for Parallel Processing},
	Date-Added = {2017-06-26 08:54:56 +0000},
	Date-Modified = {2017-07-02 13:09:34 +0000},
	Keywords = {SPIM Deconvolution;Background RealTime;GPU},
	Organization = {Springer},
	Pages = {240--250},
	Title = {Real-time deconvolution with GPU and spark for big imaging data analysis},
	Year = {2015}}

@article{temerinac2012multiview,
	Author = {Temerinac-Ott, Maja and Ronneberger, Olaf and Ochs, Peter and Driever, Wolfgang and Brox, Thomas and Burkhardt, Hans},
	Date-Added = {2017-06-26 08:50:18 +0000},
	Date-Modified = {2017-06-26 08:50:33 +0000},
	Journal = {IEEE Transactions on Image Processing},
	Keywords = {SPIM Deconvolution;Background SPIM Processing},
	Number = {4},
	Pages = {1863--1873},
	Publisher = {IEEE},
	Title = {Multiview deblurring for 3-D images from light-sheet-based fluorescence microscopy},
	Volume = {21},
	Year = {2012}}

@article{verveer2007high,
	Author = {Verveer, Peter J and Swoger, Jim and Pampaloni, Francesco and Greger, Klaus and Marcello, Marco and Stelzer, Ernst HK},
	Date-Added = {2017-06-26 08:45:31 +0000},
	Date-Modified = {2017-06-26 08:46:13 +0000},
	Journal = {Nature methods},
	Keywords = {Background SPIM Processing;SPIM Deconvolution;SPIM Pipeline},
	Number = {4},
	Pages = {311--313},
	Publisher = {Nature Publishing Group},
	Title = {High-resolution three-dimensional imaging of large specimens with light sheet--based microscopy},
	Volume = {4},
	Year = {2007}}

@article{heemskerk2015tissue,
	Author = {Heemskerk, Idse and Streichan, Sebastian J},
	Date-Added = {2017-06-26 08:39:56 +0000},
	Date-Modified = {2017-06-26 08:41:09 +0000},
	Journal = {Nature methods},
	Keywords = {Background SPIM Processing;Projection},
	Number = {12},
	Pages = {1139--1142},
	Publisher = {Nature Research},
	Title = {Tissue cartography: compressing bio-image data by dimensional reduction},
	Volume = {12},
	Year = {2015}}

@inproceedings{preibisch2008towards,
	Author = {Preibisch, Stephan and Ejsmont, Radoslaw and Rohlfing, Torsten and Tomancak, Pavel},
	Booktitle = {Biomedical Imaging: From Nano to Macro, 2008. ISBI 2008. 5th IEEE International Symposium on},
	Date-Added = {2017-06-25 19:27:00 +0000},
	Date-Modified = {2017-06-25 19:27:22 +0000},
	Keywords = {Background SPIM Processing;SPIM Pipeline},
	Organization = {IEEE},
	Pages = {324--327},
	Title = {Towards digital representation of Drosophila embryogenesis},
	Year = {2008}}

@article{preibisch2008mosaicing,
	Author = {Preibisch, Stephan and Rohlfing, Torsten and Hasak, Michael P and Tomancak, Pavel},
	Date-Added = {2017-06-25 19:25:21 +0000},
	Date-Modified = {2017-06-27 16:19:16 +0000},
	Journal = {Medical Imaging 2008: Image Processing},
	Keywords = {Background SPIM Processing;SPIM Registration;SPIM Fusion},
	Number = {1},
	Pages = {69140E},
	Publisher = {Proceedings of SPIE},
	Title = {Mosaicing of single plane illumination microscopy images using groupwise registration and fast content-based image fusion},
	Volume = {6914},
	Year = {2008}}

@article{preibisch2010software,
	Author = {Preibisch, Stephan and Saalfeld, Stephan and Schindelin, Johannes and Tomancak, Pavel},
	Date-Added = {2017-06-25 19:22:42 +0000},
	Date-Modified = {2017-06-25 19:23:00 +0000},
	Journal = {Nature methods},
	Keywords = {Background SPIM Processing;SPIM Registration},
	Number = {6},
	Pages = {418--419},
	Publisher = {Nature Publishing Group},
	Title = {Software for bead-based registration of selective plane illumination microscopy data},
	Volume = {7},
	Year = {2010}}

@article{rubio2011wavelet,
	Author = {Rubio-Guivernau, Jose L and Gurchenkov, Vasily and Luengo-Oroz, Miguel A and Duloquin, Louise and Bourgine, Paul and Santos, Andres and Peyrieras, Nadine and Ledesma-Carbayo, Maria J},
	Date-Added = {2017-06-25 19:20:20 +0000},
	Date-Modified = {2017-07-02 13:17:16 +0000},
	Journal = {Bioinformatics},
	Keywords = {Background SPIM Processing;SPIM Fusion;Spatial Scale},
	Number = {2},
	Pages = {238--245},
	Publisher = {Oxford University Press},
	Title = {Wavelet-based image fusion in multi-view three-dimensional microscopy},
	Volume = {28},
	Year = {2011}}

@article{supatto2009quantitative,
	Author = {Supatto, Willy and McMahon, Amy and Fraser, Scott E and Stathopoulos, Angelike},
	Date-Added = {2017-06-25 19:17:31 +0000},
	Date-Modified = {2017-06-25 19:17:47 +0000},
	Journal = {Nature protocols},
	Keywords = {Background SPIM Processing;SPIM Pipeline},
	Number = {10},
	Pages = {1397--1412},
	Publisher = {Nature Publishing Group},
	Title = {Quantitative imaging of collective cell migration during Drosophila gastrulation: multiphoton microscopy and computational analysis},
	Volume = {4},
	Year = {2009}}

@article{truong2011toward,
	Author = {Truong, Thai V and Supatto, Willy},
	Date-Added = {2017-06-25 19:15:44 +0000},
	Date-Modified = {2017-06-25 19:16:01 +0000},
	Journal = {genesis},
	Keywords = {SPIM Pipeline;Background DataProblem},
	Number = {7},
	Pages = {555--569},
	Publisher = {Wiley Online Library},
	Title = {Toward high-content/high-throughput imaging and analysis of embryonic morphogenesis},
	Volume = {49},
	Year = {2011}}

@article{olivier2010cell,
	Author = {Olivier, Nicolas and Luengo-Oroz, Miguel A and Duloquin, Louise and Faure, Emmanuel and Savy, Thierry and Veilleux, Isra{\"e}l and Solinas, Xavier and D{\'e}barre, Delphine and Bourgine, Paul and Santos, Andr{\'e}s and others},
	Date-Added = {2017-06-25 19:10:57 +0000},
	Date-Modified = {2017-06-27 16:16:21 +0000},
	Journal = {Science},
	Keywords = {Background SPIM;SPIM Pipeline},
	Number = {5994},
	Pages = {967--971},
	Publisher = {American Association for the Advancement of Science},
	Title = {Cell lineage reconstruction of early zebrafish embryos using label-free nonlinear microscopy},
	Volume = {329},
	Year = {2010}}

@article{krzic2012multiview,
	Author = {Krzic, Uros and Gunther, Stefan and Saunders, Timothy E and Streichan, Sebastian J and Hufnagel, Lars},
	Date-Added = {2017-06-25 19:08:27 +0000},
	Date-Modified = {2017-06-25 19:08:42 +0000},
	Journal = {Nature methods},
	Keywords = {Background SPIM Processing;SPIM Pipeline},
	Number = {7},
	Pages = {730--733},
	Publisher = {Nature Research},
	Title = {Multiview light-sheet microscope for rapid in toto imaging},
	Volume = {9},
	Year = {2012}}

@article{amat2013towards,
	Author = {Amat, Fernando and Keller, Philipp J},
	Date-Added = {2017-06-25 19:03:51 +0000},
	Date-Modified = {2017-07-06 17:04:48 +0000},
	Journal = {Development, growth \& differentiation},
	Keywords = {Background SPIM Processing;SPIM;SPIM Tracking;STB Motivation},
	Number = {4},
	Pages = {563--578},
	Publisher = {Wiley Online Library},
	Title = {Towards comprehensive cell lineage reconstructions in complex organisms using light-sheet microscopy},
	Volume = {55},
	Year = {2013}}

@article{amat2015efficient,
	Annote = {light-sheet microscopy is a powerful method for imaging the development and function of complex biological systems at high spatiotemporal resolution and over long time scales. such experiments typically generate terabytes of multidimensional image data, and thus they demand efficient computational solutions for data management, processing and analysis. We present protocols and software to tackle these steps, focusing on the imaging-based study of animal development. our protocols facilitate
(i) high-speed lossless data compression and content-based multiview image fusion optimized for multicore cpu architectures, reducing image data size 30--500-fold; (ii) automated large-scale cell tracking and segmentation; and (iii) visualization, editing and annotation of multiterabyte image data and cell-lineage reconstructions with tens of millions of data points. these software modules are open source. they provide high data throughput using a single computer workstation and are readily applicable to a wide spectrum of biological model systems.},
	Author = {Amat, Fernando and H{\"o}ckendorf, Burkhard and Wan, Yinan and Lemon, William C and McDole, Katie and Keller, Philipp J},
	Date-Added = {2017-06-25 19:02:21 +0000},
	Date-Modified = {2017-07-02 15:33:41 +0000},
	Journal = {Nature protocols},
	Keywords = {Background SPIM Processing;SPIM Pipeline;SPIM Visualization},
	Number = {11},
	Pages = {1679--1696},
	Publisher = {Nature Research},
	Title = {Efficient processing and analysis of large-scale light-sheet microscopy data},
	Volume = {10},
	Year = {2015}}

@article{stegmaier2016real,
	Author = {Stegmaier, Johannes and Amat, Fernando and Lemon, William C and McDole, Katie and Wan, Yinan and Teodoro, George and Mikut, Ralf and Keller, Philipp J},
	Date-Added = {2017-06-25 18:58:54 +0000},
	Date-Modified = {2017-07-02 15:32:55 +0000},
	Journal = {Developmental cell},
	Keywords = {Background SPIM Processing;SPIM Segmentation;SuperVoxels;SPIM Tracking;Nuclei;Cells},
	Number = {2},
	Pages = {225--240},
	Publisher = {Elsevier},
	Title = {Real-time three-dimensional cell segmentation in large-scale microscopy data of developing embryos},
	Volume = {36},
	Year = {2016}}

@article{amat2012fast,
	Author = {Amat, Fernando and Myers, Eugene W and Keller, Philipp J},
	Date-Added = {2017-06-25 18:55:13 +0000},
	Date-Modified = {2017-07-02 13:22:20 +0000},
	Journal = {Bioinformatics},
	Keywords = {Background SPIM Processing;SPIM Segmentation;GPU;IntensityScale;Nuclei},
	Number = {3},
	Pages = {373--380},
	Publisher = {Oxford University Press},
	Title = {Fast and robust optical flow for time-lapse microscopy using super-voxels},
	Volume = {29},
	Year = {2012}}

@article{preibisch2014efficient,
	Author = {Preibisch, Stephan and Amat, Fernando and Stamataki, Evangelia and Sarov, Mihail and Singer, Robert H and Myers, Eugene and Tomancak, Pavel},
	Date-Added = {2017-06-25 18:52:39 +0000},
	Date-Modified = {2017-07-02 13:06:55 +0000},
	Journal = {nature methods},
	Keywords = {Background SPIM Processing;SPIM Fusion;SPIM Deconvolution;GPU},
	Number = {6},
	Pages = {645--648},
	Publisher = {Nature Research},
	Title = {Efficient Bayesian-based multiview deconvolution},
	Volume = {11},
	Year = {2014}}

@article{amat2014fast,
	Annote = {Four major challenges complicate automated cell segmentation and tracking in advanced developmental stages. First, image data are complex, i.e., the specimen often comprises a large number of densely packed cells with different shapes and complex behav- iors. Second, image quality varies markedly across the specimen because of the limited physical penetration depth of the micro- scope. Third, data sets are large, and thus scalability is indis- pensable for any general computational approach. Fourth, high accuracy and robustness are required, because a few errors can fundamentally alter lineage results.
In general, computational approaches to cell segmentation and tracking can be divided into three main categories: contour evolu- tion (for example, level sets)21, state-space models (for example, particle filters)22 and data association methods (for example, graph matching)20,23. All of these approaches have individual strengths and weaknesses, and none solves all of the challenges outlined above. For example, contour evolution methods can be slow to compute for thousands of three-dimensional (3D) objects, but they provide detailed cell outlines, even during cell division. State-space models and data association methods do not scale well with increasing object density. However, they can model complex spatiotemporal knowledge about the system.
Here we present a new hybrid approach to segmenting and tracking cell nuclei, which facilitates fast and accurate recon- struction of cell lineages. Our method operates on three princi- ples. First, we reduce data size and complexity by using low-level segmentation to generate supervoxels. Second, we use paramet- ric contour evolution, which p},
	Author = {Amat, Fernando and Lemon, William and Mossing, Daniel P and McDole, Katie and Wan, Yinan and Branson, Kristin and Myers, Eugene W and Keller, Philipp J},
	Date-Added = {2017-06-25 18:50:25 +0000},
	Date-Modified = {2017-07-02 14:30:10 +0000},
	Journal = {Nature methods},
	Keywords = {Background SPIM Processing;SPIM Pipeline;SPIM Tracking;SPIM Segmentation;GPU;SuperVoxels},
	Publisher = {Nature Research},
	Title = {Fast, accurate reconstruction of cell lineages from large-scale fluorescence microscopy data},
	Year = {2014}}

@article{swoger2007multi,
	Author = {Swoger, Jim and Verveer, Peter and Greger, Klaus and Huisken, Jan and Stelzer, Ernst HK},
	Date-Added = {2017-06-25 18:42:47 +0000},
	Date-Modified = {2017-06-25 18:43:16 +0000},
	Journal = {Optics express},
	Keywords = {Background SPIM Processing;Spim Fusion},
	Number = {13},
	Pages = {8029--8042},
	Publisher = {Optical Society of America},
	Title = {Multi-view image fusion improves resolution in three-dimensional microscopy},
	Volume = {15},
	Year = {2007}}

@article{schmid2013high,
	Annote = {The projection onto a sphere. },
	Author = {Schmid, Benjamin and Shah, Gopi and Scherf, Nico and Weber, Michael and Thierbach, Konstantin and Campos, Citlali P{\'e}rez and Roeder, Ingo and Aanstad, Pia and Huisken, Jan},
	Date-Added = {2017-06-25 18:35:34 +0000},
	Date-Modified = {2017-06-25 18:41:00 +0000},
	Journal = {Nature communications},
	Keywords = {Background DataProblem;Background SPIM Processing;Projection},
	Publisher = {Nature Publishing Group},
	Title = {High-speed panoramic light-sheet microscopy reveals global endodermal cell dynamics},
	Volume = {4},
	Year = {2013}}

@article{weber2011light,
	Annote = {Only recently have microscope technology and computing power been able to keep up with the desire of researchers to follow tissue movements, to track every individual cell and to quantify what is seen in time-lapse movies and image stacks. The `transparent embryo' has not yet become a reality due to the persistent limitations of microscopes such as low penetration, photo-toxicity, slow acquisition, and sample incompatibility.

Further, once the desired data have been successfully acquired, it is then a challenge to turn images and movies into accurate quantitation. The ultimate goal is to model the ideal embryo that incorporates all the details observed with modern imaging equipment.},
	Author = {Weber, Michael and Huisken, Jan},
	Date-Added = {2017-06-25 16:13:17 +0000},
	Date-Modified = {2017-07-02 15:48:07 +0000},
	Journal = {Current opinion in genetics \& development},
	Keywords = {SPIM Fusion;SPIM Pipeline;Background SPIM Processing;Background RealTime},
	Number = {5},
	Pages = {566--572},
	Publisher = {Elsevier},
	Title = {Light sheet microscopy for real-time developmental biology},
	Volume = {21},
	Year = {2011}}

@article{scherf2015smart,
	Annote = {Data Problem:

With an ideal, minimally invasive micro- scope, the health of the biological specimen is no longer the speed-limiting factor; the sample can be imaged as fast as technically possible, for as long as needed. However, this freedom would create another techni-cal challenge and bottleneck: the large size of the acquired data. Modern light-sheet techniques generate an enormous volume of images (many terabytes per day) that need to be transferred, stored and analyzed, putting pressure on the limits of even state-of-the-art infrastructures. Big image data have limited most experiments to a single sample and pre- vented systematic analysis of large ensembles. Although dedicated hardware and software might alleviate the problem9, treating acquisi- tion and processing as mutually independent is no longer a viable solution. By acquiring blindly and processing later, we obtain image data that are vastly larger than the specific information sought from the sample (e.g., dynamic tracks of cell movements and average fluorescent level per cell). Instead, we need a flexible, smart acquisition concept that deliv- ers information-rich data with minimal over- head. 

As an intrinsic benefit the sample would be less exposed through more discriminatory data acquisition. Achieving this will require feedback between imaging and data process- ing to record only what is necessary. Ideally this should happen right at the source---in the microscope, at the camera.

Real Time:

Intelligent feedback between control, acqui- sition and processing is a basic requirement of such an approach. It provides the flexibility to optimally use all available resources and to get the most information from each experiment within our time constraints. Closed-loop feedbacks have proven useful for imaging in research and in industry for different pur- poses including autofocus, automation of screening experiments10 or quality control. Adaptive optics yielded impressive improve- ments in image quality in astronomy and more recently in biology11, particularly for deep tissue imaging. However, for systematic in vivo microscopy it is not insufficient qual- ity, but the sheer amount of image data, that poses the primary challenge. of adaptive acquisition has to be extended to selectively collect only data of interest in the first place, tailored to the particular question at hand.},
	Author = {Scherf, Nico and Huisken, Jan},
	Date-Added = {2017-06-25 15:38:41 +0000},
	Date-Modified = {2017-06-26 09:29:47 +0000},
	Journal = {Nature biotechnology},
	Keywords = {Background RealTime;Background DataProblem;Background SPIM Processing;SPIM SmartM},
	Number = {8},
	Pages = {815--818},
	Publisher = {Nature Research},
	Title = {The smart and gentle microscope},
	Volume = {33},
	Year = {2015}}

@article{weber2012omnidirectional,
	Annote = {The great challenge now is to process the vast quantities of SPIM images in about the same amount of time it takes to acquire the data. Increasing the image acquisition speed further requires a corresponding increase in processing power. It remains to be seen how multiview data sets for more complex specimens such as older fruit fly embryos or the much larger zebrafish can be analyzed to a meaningful extent in a reasonable amount of time. Optimally, the microscope should deliver application-specific, meaningful information such as cell tracks and genealogy in real time rather than raw image data.

},
	Author = {Weber, Michael and Huisken, Jan},
	Date-Added = {2017-06-25 15:29:26 +0000},
	Date-Modified = {2017-06-25 15:30:45 +0000},
	Journal = {nature methods},
	Keywords = {Background SPIM;Background DataProblem;Background RealTime},
	Number = {7},
	Pages = {656--657},
	Publisher = {Nature Research},
	Title = {Omnidirectional microscopy},
	Volume = {9},
	Year = {2012}}

@article{tomer2012quantitative,
	Annote = {In addition to optomechanical challenges, Tomer et al.4 offer a solution to another critical aspect of long-term, high-speed imaging: the data-acquisition rates of the latest cameras exceed the speed and capacity of common data storage solutions. Real-time fusion and compression eliminate the need to write all the raw image data to hard drives, keeping imaging speeds high. They performed the slower segmentation and tracking task for the rapid early fly morphogenesis in a postprocessing step.

},
	Author = {Tomer, Raju and Khairy, Khaled and Amat, Fernando and Keller, Philipp J},
	Date-Added = {2017-06-25 15:25:01 +0000},
	Date-Modified = {2017-06-25 15:26:51 +0000},
	Journal = {Nature Methods},
	Keywords = {Background SPIM Processing;SPIM Pipeline;SPIM Fusion},
	Number = {7},
	Pages = {755--763},
	Publisher = {Nature Publishing Group},
	Title = {Quantitative high-speed imaging of entire developing embryos with simultaneous multiview light-sheet microscopy},
	Volume = {9},
	Year = {2012}}

@article{reynaud2015guide,
	Annote = {Another strategy is to avoid acquiring new data until the data at hand have been processed. Raw data could even be deleted after processing, since it may be cheaper to repeat the experiment than to store the massive amount of raw data. However, most biologists are likely to be reluctant to follow this strategy; who would want to delete spectacular recordings and blindly trust processing pipelines that are far from per- fect? There may also be legal considerations: in Germany, for example, raw data must be kept for 10 years by law. It is time to think about what exactly ``raw data'' means in the context of light-sheet microscopy.
The most promising approach for tam- ing the data deluge is to employ clever compression strategies. One can reduce the dimensionality of the data on the fly---for example, by transforming 3D stacks to 2D cartographic projections8---although such solutions typically cannot be easily gener-alized. Existing compression algorithms, although able to reduce raw data volumes impressively, come with a cost in regard to interactive access to the data. An ideal scheme would be fast and reduce the data to a fraction of its original volume using the temporal dimension. Although such com- pression schemes are under intense research in computer science labs, biologists have to get seriously geeky to be able to handle light- sheet microscopy data sets. Information technology (IT) is barely on the curriculum of most biology departments, and yet in the absence of professional IT support, which is often lacking, the burden will fall on biolo- gists. Fortunately, IT companies, such as Acquifer, see a business opportunity in help- ing biologists shuffle their piles of light-sheet imagery.},
	Author = {Reynaud, Emmanuel G and Peychl, Jan and Huisken, Jan and Tomancak, Pavel},
	Date-Added = {2017-06-25 14:15:33 +0000},
	Date-Modified = {2017-07-01 18:59:22 +0000},
	Journal = {Nature methods},
	Keywords = {Background SPIM;Background SPIM Processing;Background DataProblem},
	Number = {1},
	Pages = {30--34},
	Publisher = {Nature Research},
	Title = {Guide to light-sheet microscopy for adventurous biologists},
	Volume = {12},
	Year = {2015}}

@article{chen2014lattice,
	Author = {Chen, Bi-Chang and Legant, Wesley R and Wang, Kai and Shao, Lin and Milkie, Daniel E and Davidson, Michael W and Janetopoulos, Chris and Wu, Xufeng S and Hammer, John A and Liu, Zhe and others},
	Date-Added = {2017-06-25 13:58:24 +0000},
	Date-Modified = {2017-06-25 13:59:21 +0000},
	Journal = {Science},
	Keywords = {Background SPIM;Background Microscope},
	Number = {6208},
	Pages = {1257998},
	Publisher = {American Association for the Advancement of Science},
	Title = {Lattice light-sheet microscopy: imaging molecules to embryos at high spatiotemporal resolution},
	Volume = {346},
	Year = {2014}}

@article{huisken2004optical,
	Annote = {Original Jan Huisken Spim Paper},
	Author = {Huisken, Jan and Swoger, Jim and Del Bene, Filippo and Wittbrodt, Joachim and Stelzer, Ernst HK},
	Date-Added = {2017-06-25 13:50:10 +0000},
	Date-Modified = {2017-06-25 13:58:15 +0000},
	Journal = {Science},
	Keywords = {Background Microscope;Background SPIM},
	Number = {5686},
	Pages = {1007--1009},
	Publisher = {American Association for the Advancement of Science},
	Title = {Optical sectioning deep inside live embryos by selective plane illumination microscopy},
	Volume = {305},
	Year = {2004}}

@article{keller2008reconstruction,
	Annote = {Original DSLM paper.},
	Author = {Keller, Philipp J and Schmidt, Annette D and Wittbrodt, Joachim and Stelzer, Ernst HK},
	Date-Added = {2017-06-25 13:47:45 +0000},
	Date-Modified = {2017-06-25 13:50:00 +0000},
	Journal = {Science},
	Keywords = {Background SPIM;Background Microscope},
	Number = {5904},
	Pages = {1065--1069},
	Publisher = {American Association for the Advancement of},
	Title = {Reconstruction of zebrafish early embryonic development by scanned light sheet microscopy},
	Volume = {322},
	Year = {2008}}

@article{power2017guide,
	Annote = {Provides comprehensive review of research into the field. 

Here we give an overview of the state of the LSFM field, providing critical appraisal of the various advances, and note the need for vigilance in ensuring the primacy of the biological system in driving decisions. },
	Author = {Power, Rory M and Huisken, Jan},
	Date-Added = {2017-06-25 13:32:06 +0000},
	Date-Modified = {2017-06-25 13:36:33 +0000},
	Journal = {Nature Methods},
	Keywords = {Background SPIM;},
	Number = {4},
	Pages = {360--373},
	Publisher = {Nature Research},
	Title = {A guide to light-sheet fluorescence microscopy for multiscale imaging},
	Volume = {14},
	Year = {2017}}

@article{dyn2002adaptive,
	Annote = {Original paper for introduction of adaptive thinning.

Complexity NlogN},
	Author = {Dyn, Nira and Floater, Michael S and Iske, Armin},
	Date-Added = {2017-06-23 10:44:17 +0000},
	Date-Modified = {2017-06-23 10:48:39 +0000},
	Journal = {Journal of Computational and Applied Mathematics},
	Keywords = {Background IMesh;Background Content;Adaptive Thinning},
	Number = {2},
	Pages = {505--517},
	Publisher = {Elsevier},
	Title = {Adaptive thinning for bivariate scattered data},
	Volume = {145},
	Year = {2002}}

@article{demaret2002scattered,
	Annote = {Complexity NlogN},
	Author = {Demaret, Laurent and Iske, Armin},
	Date-Added = {2017-06-23 10:37:29 +0000},
	Date-Modified = {2017-06-23 10:48:20 +0000},
	Journal = {Curve and Surface Fitting: Saint-Malo},
	Keywords = {Background Content;Background IMesh;Adaptive Thinning},
	Pages = {107--117},
	Title = {Scattered data coding in digital image compression},
	Volume = {2003},
	Year = {2002}}

@article{said1996new,
	Annote = {Very good compression algorithm using wavelets.

Uses ideas of zero-trees. 

Ordering of a wavelet transforms co-efficients, in smart ways using trees to structure them to improve the comrpession rate. Aprentlly very fast. },
	Author = {Said, Amir and Pearlman, William A},
	Date-Added = {2017-06-23 09:51:07 +0000},
	Date-Modified = {2017-06-23 09:58:55 +0000},
	Journal = {IEEE Transactions on circuits and systems for video technology},
	Keywords = {Background Content;Background Wavelets;Background Tree},
	Number = {3},
	Pages = {243--250},
	Publisher = {IEEE},
	Title = {A new, fast, and efficient image codec based on set partitioning in hierarchical trees},
	Volume = {6},
	Year = {1996}}

@article{sarkis2009content,
	Annote = {Provides a recent overview.

Triangular Mesh:

 This work presents a novel method based on Binary Space Partitions in combination with three clustering schemes to approximate an image with a mesh

If a triangle's equation does not have the ability to reconstruct the pixels lying within up to a predefined error, it is split into two new triangles. Tested on several real images, the proposed method leads to reduced size meshes in a fast manner while retaining the visual quality of the reconstructed images. In addition, it is parallelizable due to the property of Binary Space Partitions which facilitates its application in real-time scenarios.

The main idea behind this work is that each triangle of the mesh can be uniquely described with the plane defined by its three vertices. In this way, a triangle of the mesh is ob- tained while its nodes are the sought nonuniform samples. If any of the obtained triangles, cannot represent the points that lie within up to a predefined error metric, it is further split into two smaller triangles until the error is satisfied across the image. Three versions of the meshing scheme are derived based on the methodology used to split each triangle. The first one is based on the tritree concept which equally divides each triangle into two smaller ones. The second is based on the kmeans clustering technique which splits a triangle by finding the centers of the two main clusters of the pixels [24].},
	Author = {Sarkis, Michel and Diepold, Klaus},
	Date-Added = {2017-06-23 09:20:49 +0000},
	Date-Modified = {2017-06-23 11:43:40 +0000},
	Journal = {IEEE Transactions on Image Processing},
	Keywords = {Background Content;Background IMesh},
	Number = {5},
	Pages = {1069--1079},
	Publisher = {IEEE},
	Title = {Content adaptive mesh representation of images using binary space partitions},
	Volume = {18},
	Year = {2009}}

@article{demaret2004advances,
	Annote = {Greedy Point Removal Method (GPR) as described by Adams, or Adaptive thinning. 

Ideas coming from scattered data approimation, point removal.

Intoduced in compression view-point.

Introduced as an alternative for wavelets, as they do noto handle edges well. 

Attemps to minimize the MSE, by remove individual points one by one. 

Has a criteria of minimal point removal that looks like the resolution bound. 

Complexity NlogN
},
	Author = {Demaret, Laurent and Iske, Armin and others},
	Date-Added = {2017-06-23 09:17:22 +0000},
	Date-Modified = {2017-06-23 10:48:30 +0000},
	Journal = {Annals of the MCFA},
	Keywords = {Background Content;Background IMesh;Adaptive Thinning},
	Pages = {105--109},
	Title = {Advances in digital image compression by adaptive thinning},
	Volume = {3},
	Year = {2004}}

@article{wang1996use,
	Annote = {The paper used as a quad-tree mesh generation reference in Yang , et al. mesh generation. },
	Author = {Wang, Yao and Lee, Ouseb and Vetro, Anthony},
	Date-Added = {2017-06-23 08:52:27 +0000},
	Date-Modified = {2017-06-23 08:53:01 +0000},
	Journal = {IEEE Transactions on circuits and systems for video technology},
	Keywords = {Background Content;Background IMesh;Background Tree},
	Number = {6},
	Pages = {647--659},
	Publisher = {IEEE},
	Title = {Use of two-dimensional deformable mesh structures for video coding. II. The analysis problem and a region-based coder employing an active mesh representation},
	Volume = {6},
	Year = {1996}}

@inproceedings{floyd1976adaptive,
	Annote = {Use by many algorithms for adaptive placement of pixels proportional to image intensity},
	Author = {Floyd, Robert W},
	Booktitle = {Proc. Soc. Inf. Disp.},
	Date-Added = {2017-06-23 08:38:16 +0000},
	Date-Modified = {2017-06-23 08:41:30 +0000},
	Keywords = {Background Content;Background IMesh;Error Diffusion},
	Pages = {75--77},
	Title = {An adaptive algorithm for spatial gray-scale},
	Volume = {17},
	Year = {1976}}

@article{yang2003fast,
	Annote = {Traingulations produced using Floyd-Steinberg error-diffusion algorithm for placement. 

Triangulations and comes to an error bound relating the upper bound on the second gradient over the linear element. 

Places points according to a monitor function that is proportional to the second derivative of the function. 

\sigma = (\frac{G(x,y)}{A})^\gamma

where G(x,y) is the magitue fo the largest second derivative. 

Mentions previous approaches Quadtree and Mesh optimization had run times of 10seconds to an hour. },
	Author = {Yang, Yongyi and Wernick, Miles N and Brankov, Jovan G},
	Date-Added = {2017-06-23 08:28:59 +0000},
	Date-Modified = {2017-06-23 08:42:25 +0000},
	Journal = {IEEE transactions on image processing},
	Keywords = {Background Content;Background IMesh},
	Number = {8},
	Pages = {866--881},
	Publisher = {IEEE},
	Title = {A fast approach for accurate content-adaptive mesh generation},
	Volume = {12},
	Year = {2003}}

@article{adams2011flexible,
	Annote = {Greedy Point Removal improvement of Demaret and Isky. Using the ED, method.

Delaunay Triangulation:

The idea is to first do a cheaper scheme to get a sub-optimal solution, then to improve on this using the GPR method, which is more expsive, but you have reduced the steps to only over the remain Np number of points instead of N.

Time is from .2 -7 seconds on 512x512

Reduces complexity of memory by only requiring the triangulation to be formed from Np, instead of N.
},
	Author = {Adams, Michael D},
	Date-Added = {2017-06-23 08:13:05 +0000},
	Date-Modified = {2017-06-23 12:40:51 +0000},
	Journal = {IEEE Transactions on Image Processing},
	Keywords = {Background Content;Background IMesh},
	Number = {9},
	Pages = {2414--2427},
	Publisher = {IEEE},
	Title = {A flexible content-adaptive mesh-generation strategy for image representation},
	Volume = {20},
	Year = {2011}}

@book{saucez2001adaptive,
	Annote = {Adapting resolution compared to a monitor function. },
	Author = {Saucez, Ph and Schiesser, WE and others},
	Date-Added = {2017-06-22 14:34:40 +0000},
	Date-Modified = {2017-06-22 14:35:06 +0000},
	Keywords = {Background Equidistribution;Background PDE},
	Publisher = {CRC Press},
	Title = {Adaptive method of lines},
	Year = {2001}}

@article{awile2012fast,
	Author = {Awile, Omar and B{\"u}y{\"u}kke{\c{c}}eci, Ferit and Reboux, Sylvain and Sbalzarini, Ivo F},
	Date-Added = {2017-06-22 14:25:08 +0000},
	Date-Modified = {2017-06-22 14:25:22 +0000},
	Journal = {Computer Physics Communications},
	Keywords = {Background Equidistribution},
	Number = {5},
	Pages = {1073--1081},
	Publisher = {North-Holland},
	Title = {Fast neighbor lists for adaptive-resolution particle simulations},
	Volume = {183},
	Year = {2012}}

@article{schrader2010discretization,
	Author = {Schrader, Birte and Reboux, Sylvain and Sbalzarini, Ivo F},
	Date-Added = {2017-06-22 14:13:30 +0000},
	Date-Modified = {2017-06-22 14:13:36 +0000},
	Journal = {Journal of Computational Physics},
	Keywords = {DCPSE},
	Number = {11},
	Pages = {4159--4182},
	Publisher = {Academic Press},
	Title = {Discretization correction of general integral PSE Operators for particle methods},
	Volume = {229},
	Year = {2010}}

@article{reboux2012self,
	Author = {Reboux, Sylvain and Schrader, Birte and Sbalzarini, Ivo F},
	Date-Added = {2017-06-22 13:50:37 +0000},
	Date-Modified = {2017-06-22 14:13:27 +0000},
	Journal = {Journal of Computational Physics},
	Keywords = {Background Equidistribution;Background PDE;Continuous;DCPSE},
	Number = {9},
	Pages = {3623--3646},
	Publisher = {Elsevier},
	Title = {A self-organizing Lagrangian particle method for adaptive-resolution advection--diffusion simulations},
	Volume = {231},
	Year = {2012}}

@article{pereyra1974mesh,
	Annote = {Uses the equidsitribution ideas from Deboor for splines and applies them to the solution of ODE's},
	Author = {Pereyra, V and Sewell, EG},
	Date-Added = {2017-06-22 12:38:03 +0000},
	Date-Modified = {2017-06-22 13:47:57 +0000},
	Journal = {Numerische Mathematik},
	Keywords = {Background Equidistribution;Background ODE},
	Number = {3},
	Pages = {261--268},
	Publisher = {Springer},
	Title = {Mesh selection for discrete solution of boundary problems in ordinary differential equations},
	Volume = {23},
	Year = {1974}}

@article{adcock2017infinite,
	Annote = {Describes the problem as a l1 minimization of the function approximation.

Discusses problems with other approaches

Recent. },
	Author = {Adcock, Ben},
	Date-Added = {2017-06-22 09:33:30 +0000},
	Date-Modified = {2017-06-22 09:40:19 +0000},
	Journal = {Constructive Approximation},
	Keywords = {Recent;Reconstruction Condition},
	Number = {3},
	Pages = {345--390},
	Publisher = {Springer},
	Title = {Infinite-Dimensional$\backslash$ell\^{} 1 Minimization and Function Approximation from Pointwise Data},
	Volume = {45},
	Year = {2017}}

@incollection{deslauriers1989symmetric,
	Author = {Deslauriers, Gilles and Dubuc, Serge},
	Booktitle = {Constructive approximation},
	Date-Added = {2017-06-22 09:28:57 +0000},
	Date-Modified = {2017-06-22 09:29:20 +0000},
	Keywords = {Background Wavelets;Reconstruction Functions},
	Pages = {49--68},
	Publisher = {Springer},
	Title = {Symmetric iterative interpolation processes},
	Year = {1989}}

@techreport{babuska1995partition,
	Annote = {The partition of unity reconstruciton is basically the condition I have, it is exactly it once you have allow for the more general addition of the inclusion of |\xi_p|

Has similar error estimates following the legrange form as well. },
	Author = {Babuska, Ivo and Melenk, Jens M},
	Date-Added = {2017-06-22 07:00:33 +0000},
	Date-Modified = {2017-06-22 09:15:04 +0000},
	Institution = {DTIC Document},
	Keywords = {Background POU;Background PDE;Resolution Bound;Reconstruction Functions},
	Title = {The partition of unity finite element method},
	Year = {1995}}

@article{babuvska1992h,
	Author = {Babu{\v{s}}ka, Ivo and Guo, BQ},
	Date-Added = {2017-06-22 06:43:31 +0000},
	Date-Modified = {2017-06-22 07:59:44 +0000},
	Journal = {Advances in Engineering Software},
	Keywords = {Background hpFEM;Background PDE},
	Number = {3-4},
	Pages = {159--174},
	Publisher = {Elsevier},
	Title = {The h, p and hp version of the finite element method; basis theory and applications},
	Volume = {15},
	Year = {1992}}

@incollection{de1973good,
	Annote = {Introduces the error equidistribution principle, and the error bounds that I use. 

Mentions that the approach had already been tried for solving pde's showing computational speed ups and improvement of error.

},
	Author = {de Boor, Carl},
	Booktitle = {Spline functions and approximation theory},
	Date-Added = {2017-06-22 06:11:55 +0000},
	Date-Modified = {2017-06-22 12:37:42 +0000},
	Keywords = {Background Splines;Background PDE;Background Statistics;Background Equidistribution},
	Pages = {57--72},
	Publisher = {Springer},
	Title = {Good approximation by splines with variable knots},
	Year = {1973}}

@inproceedings{de1974good,
	Annote = {Simple equidistribution scheme, aparentlly doesn't extend well beyond 1D.},
	Author = {De Boor, Carl},
	Booktitle = {Conference on the numerical solution of differential equations},
	Date-Added = {2017-06-21 17:37:17 +0000},
	Date-Modified = {2017-06-21 17:38:06 +0000},
	Keywords = {Background Equidistribution;Background Splines;Background Statistics},
	Organization = {Springer},
	Pages = {12--20},
	Title = {Good approximation by splines with variable knots. II},
	Year = {1974}}

@article{burchard1974splines,
	Annote = {Aparentlly the original equidistribution idea paper. 

Has forms of the resolution bound for splines. 

Derives the resolution bound more or less explicitly. 

Statement on optimality vs. polynomials.

Doesn't seem to use it to show how to place h though},
	Author = {Burchard, Hermann G},
	Date-Added = {2017-06-21 17:34:23 +0000},
	Date-Modified = {2017-06-22 10:37:07 +0000},
	Journal = {Applicable Analysis},
	Keywords = {Background Equidistribution;Background Statistics;Background Splines;Resolution Bound},
	Number = {4},
	Pages = {309--319},
	Publisher = {Taylor \& Francis},
	Title = {Splines (with optimal knots) are better},
	Volume = {3},
	Year = {1974}}

@book{huang2010adaptive,
	Author = {Huang, Weizhang and Russell, Robert D},
	Date-Added = {2017-06-21 17:14:06 +0000},
	Date-Modified = {2017-06-22 14:25:44 +0000},
	Keywords = {Background MMPDE;Background PDE;Review;Background Equidistribution},
	Publisher = {Springer Science \& Business Media},
	Title = {Adaptive moving mesh methods},
	Year = {2010}}

@article{huang1994moving,
	Author = {Huang, Weizhang and Ren, Yuhe and Russell, Robert D},
	Date-Added = {2017-06-21 17:08:26 +0000},
	Date-Modified = {2017-06-21 17:08:47 +0000},
	Journal = {SIAM Journal on Numerical Analysis},
	Keywords = {Background PDE;Background MMPDE},
	Number = {3},
	Pages = {709--730},
	Publisher = {SIAM},
	Title = {Moving mesh partial differential equations (MMPDES) based on the equidistribution principle},
	Volume = {31},
	Year = {1994}}

@article{babuvska1979direct,
	Annote = {Aparentlly first work, the paper is a bit dense, and hard to get into. },
	Author = {Babu{\v{s}}ka, Ivo and Kellogg, RB and Pitk{\"a}ranta, J},
	Date-Added = {2017-06-21 15:09:01 +0000},
	Date-Modified = {2017-06-21 16:11:29 +0000},
	Journal = {Numerische Mathematik},
	Keywords = {Background AFEM;Background PDE},
	Number = {4},
	Pages = {447--471},
	Publisher = {Springer},
	Title = {Direct and inverse error estimates for finite elements with mesh refinements},
	Volume = {33},
	Year = {1979}}

@incollection{nochetto2011primer,
	Annote = {Despite their practical success, adaptive processes have been shown to converge, and to exhibit optimal cardinality, only recently for dimension d > 1 and for linear elliptic PDE

Equidistribution principle, and Legrange error, also a resolution like condition. Instead the problem is formulated as given N elements, how do you minimize this error.

The conclusion is then to equidistribute the error across the N elements.

Equidistribution principle, based on second derivative, but posses it in a different way, basically how can you minimize the error given $N$ terms. },
	Author = {Nochetto, Ricardo H and Veeser, Andreas},
	Booktitle = {Multiscale and adaptivity: modeling, numerics and applications},
	Date-Added = {2017-06-21 14:36:24 +0000},
	Date-Modified = {2017-06-21 15:16:51 +0000},
	Keywords = {Review;Background PDE;Background AFEM},
	Pages = {125--225},
	Publisher = {Springer},
	Title = {Primer of adaptive finite element methods},
	Year = {2011}}

@article{schneider2010wavelet,
	Annote = {Provides a review of wavelet methods in computational fluid dynamics. },
	Author = {Schneider, Kai and Vasilyev, Oleg V},
	Date-Added = {2017-06-21 13:13:48 +0000},
	Date-Modified = {2017-06-21 13:14:17 +0000},
	Journal = {Annual Review of Fluid Mechanics},
	Keywords = {Background Wavelets;Background PDE;Wavelet PDE;Review},
	Pages = {473--503},
	Publisher = {Annual Reviews},
	Title = {Wavelet methods in computational fluid dynamics},
	Volume = {42},
	Year = {2010}}

@article{vasilyev2000second,
	Annote = {Argue that second generation wavelets provide superior properties in terms of the adaptation, at differnet levels and response, then just straight using first generation interpolating wavelet transforms. 

That then the Petros paper they argue is added complexity and not needed. 

Argue that this then allows the localisation of co-efficients to then have a more physical interporetation that can then be used for development of algorithms.

The response of a dirac delta looks similar to an APR's response actually. 

pg. 671

Perfect Reconstruction Criteria, that adds in at higher levels, + then addition of padding or neighbours at other resolution levels based on criteria of adjacent zones. 

pg. 676

Discusses how the derivatives are done, utilizing the wavelet tree structure.

This is likely the closest to the APR, as it discusses how it can be exended to complex geometries, higher dimensions, and also can use the criteria for bounding the derivatives by performing additional wavelet analysis.


},
	Author = {Vasilyev, Oleg V and Bowman, Christopher},
	Date-Added = {2017-06-21 10:26:07 +0000},
	Date-Modified = {2017-06-21 11:58:44 +0000},
	Journal = {Journal of Computational Physics},
	Keywords = {Background Wavelets;Background PDE;Wavelet PDE;Pulling Scheme},
	Number = {2},
	Pages = {660--693},
	Publisher = {Elsevier},
	Title = {Second-generation wavelet collocation method for the solution of partial differential equations},
	Volume = {165},
	Year = {2000}}

@article{harten1994adaptive,
	Annote = {Aparentlly this is the original wavelet pde solver. 

Uses and interpolative scheme between levels.

Computes the derative at all levels where the detail co-efficient is required and propogates down, all other cases, uses interpolation.

Error proportional to epsilon. 

Nested grid structure.

Sets the support of the interpolant to all be included at the resolution as well. (Getting the neighbourhood)

},
	Author = {Harten, Ami},
	Date-Added = {2017-06-21 08:17:54 +0000},
	Date-Modified = {2017-06-21 09:34:23 +0000},
	Journal = {Journal of Computational Physics},
	Keywords = {Background Wavelets;Background PDE;Wavelet PDE},
	Number = {2},
	Pages = {319--338},
	Publisher = {Elsevier},
	Title = {Adaptive multiresolution schemes for shock computations},
	Volume = {115},
	Year = {1994}}

@article{regele2009adaptive,
	Author = {Regele, JD and Vasilyev, OV},
	Date-Added = {2017-06-21 07:48:20 +0000},
	Date-Modified = {2017-06-21 07:48:41 +0000},
	Journal = {International Journal of Computational Fluid Dynamics},
	Keywords = {Background PDE;Background Wavelets;Wavelet PDE},
	Number = {7},
	Pages = {503--518},
	Publisher = {Taylor \& Francis},
	Title = {An adaptive wavelet-collocation method for shock computations},
	Volume = {23},
	Year = {2009}}

@article{hejazialhosseini2010high,
	Author = {Hejazialhosseini, Babak and Rossinelli, Diego and Bergdorf, Michael and Koumoutsakos, Petros},
	Date-Added = {2017-06-20 19:15:43 +0000},
	Date-Modified = {2017-06-20 19:16:01 +0000},
	Journal = {Journal of Computational Physics},
	Keywords = {Background PDE;Background Wavelets},
	Number = {22},
	Pages = {8364--8383},
	Publisher = {Elsevier},
	Title = {High order finite volume methods on wavelet-adapted grids with local time-stepping on multicore architectures for the simulation of shock-bubble interactions},
	Volume = {229},
	Year = {2010}}

@article{berger1989local,
	Annote = {This is credited as being the original AMR paper in wikipedia. 

Adaptive schemes for solving hyperbolic conversation laws.

Highlights the problemw with adaptive methods is the need for data structure not usually found in numerical software.

Tries to mintain a fixed accuracy for minimum cost.

Discusses whether expensive schemes are still needed with adaptive methods, they answer yes. But also say that for the expensive type of schemes, adaptive methods becomes more beneficial.

Allow computations tht woudln't have otherwise been possible.

Meshs are nested, that is higher resolution meshs are always contained in the lower ones.

An estimation of the time stepping error is then used., by using a different between two solutions.

Grid  points are flagged as needing higher resolution.

And algorithm goes through and tries to group them into regular grids.

These grids are then split, if they are getting to little hot points.},
	Author = {Berger, Marsha J and Colella, Phillip},
	Date-Added = {2017-06-20 14:03:09 +0000},
	Date-Modified = {2017-06-20 14:37:25 +0000},
	Journal = {Journal of computational Physics},
	Keywords = {Background PDE;AMR},
	Number = {1},
	Pages = {64--84},
	Publisher = {Elsevier},
	Title = {Local adaptive mesh refinement for shock hydrodynamics},
	Volume = {82},
	Year = {1989}}

@article{almgren1998conservative,
	Annote = {This doesn't seem to be the original, not sure If I shoudl cite it, have now added the original.},
	Author = {Almgren, Ann S and Bell, John B and Colella, Phillip and Howell, Louis H and Welcome, Michael L},
	Date-Added = {2017-06-20 13:40:25 +0000},
	Date-Modified = {2017-06-20 14:02:50 +0000},
	Journal = {Journal of computational Physics},
	Keywords = {Background PDE;AMR},
	Number = {1},
	Pages = {1--46},
	Publisher = {Elsevier},
	Title = {A conservative adaptive projection method for the variable density incompressible Navier--Stokes equations},
	Volume = {142},
	Year = {1998}}

@article{rossinelli2015mrag,
	Annote = {Uses the interpolating wavelets from Donoho.

Comments on how the trade-off between higher adaption and actual execution time increase, and the complication of data-structures},
	Author = {Rossinelli, Diego and Hejazialhosseini, Babak and van Rees, Wim and Gazzola, Mattia and Bergdorf, Michael and Koumoutsakos, Petros},
	Date-Added = {2017-06-20 13:36:52 +0000},
	Date-Modified = {2017-06-21 07:49:04 +0000},
	Journal = {Journal of Computational Physics},
	Keywords = {Background Wavelets;Background PDE;Wavelet PDE},
	Pages = {1--18},
	Publisher = {Elsevier},
	Title = {MRAG-I2D: multi-resolution adapted grids for remeshed vortex methods on multicore architectures},
	Volume = {288},
	Year = {2015}}

@article{donoho1992interpolating,
	Annote = {Minimax Bound - Reconstruction Condition.

They show optimal convergence, and relationships between some known classes and a constant for specific epsilon. But do not provide exact reconstruction values.

That is they do not provide an exact threshold to maintain a certain Minimax bound.

},
	Author = {Donoho, David L},
	Date-Added = {2017-06-20 09:48:30 +0000},
	Date-Modified = {2017-06-20 13:11:16 +0000},
	Journal = {Preprint, Department of Statistics, Stanford University},
	Keywords = {Background Wavelets;Wavelet PDE;Reconstruction Condition},
	Number = {3},
	Title = {Interpolating wavelet transforms},
	Volume = {2},
	Year = {1992}}

@article{unser1996review,
	Author = {Unser, Michael and Aldroubi, Akram},
	Date-Added = {2017-06-18 14:25:53 +0000},
	Date-Modified = {2017-06-18 14:26:01 +0000},
	Journal = {Proceedings of the IEEE},
	Keywords = {Background Wavelets},
	Number = {4},
	Pages = {626--638},
	Publisher = {IEEE},
	Title = {A review of wavelets in biomedical applications},
	Volume = {84},
	Year = {1996}}

@article{achanta2012slic,
	Author = {Achanta, Radhakrishna and Shaji, Appu and Smith, Kevin and Lucchi, Aurelien and Fua, Pascal and S{\"u}sstrunk, Sabine},
	Date-Added = {2017-06-17 13:57:48 +0000},
	Date-Modified = {2017-06-17 13:58:03 +0000},
	Journal = {IEEE transactions on pattern analysis and machine intelligence},
	Keywords = {Background Superpixels;Review},
	Number = {11},
	Pages = {2274--2282},
	Publisher = {IEEE},
	Title = {SLIC superpixels compared to state-of-the-art superpixel methods},
	Volume = {34},
	Year = {2012}}

@article{davis1997adaptive,
	Annote = {General approximation by dictionaries. Finding the optimal basis that satisfies my reconstruction condition for redundant dictionaries is an NP-Hard problem.

They are general, and not necessarilly localized in space, although, I guess they can be. 

Calls the problem of finding a (\epsilon, M) approximation},
	Author = {Davis, Geoff and Mallat, Stephane and Avellaneda, Marco},
	Date-Added = {2017-06-17 13:17:25 +0000},
	Date-Modified = {2017-07-09 10:49:11 +0000},
	Journal = {Constructive approximation},
	Keywords = {Background General;NP-Hard;Reconstruction Condition;Dictionary},
	Number = {1},
	Pages = {57--98},
	Publisher = {Springer},
	Title = {Adaptive greedy approximations},
	Volume = {13},
	Year = {1997}}

@article{mallat1989theory,
	Annote = {One of the original papers in wavelets},
	Author = {Mallat, Stephane G},
	Date-Added = {2017-06-17 11:37:38 +0000},
	Date-Modified = {2017-06-17 11:37:49 +0000},
	Journal = {IEEE transactions on pattern analysis and machine intelligence},
	Keywords = {Background Wavelets},
	Number = {7},
	Pages = {674--693},
	Publisher = {Ieee},
	Title = {A theory for multiresolution signal decomposition: the wavelet representation},
	Volume = {11},
	Year = {1989}}

@inproceedings{witkin1984scale,
	Annote = {Scale space representations, expand from 1D, to a 2D, with a scale space added for each point, which reflects a different scale blurred image. 

Form this then extrema are extracted, and then zero contours used, to associate and link parts of space of the function.

These can then be descritized to sub-division the scale space further. 


},
	Author = {Witkin, Andrew},
	Booktitle = {Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP'84.},
	Date-Added = {2017-06-17 11:05:00 +0000},
	Date-Modified = {2017-06-17 11:25:58 +0000},
	Keywords = {Background Tree;},
	Organization = {IEEE},
	Pages = {150--153},
	Title = {Scale-space filtering: A new approach to multi-scale description},
	Volume = {9},
	Year = {1984}}

@article{burt1983laplacian,
	Annote = {Guassian and Laplacian difference pyramids, similar to, wavelets},
	Author = {Burt, Peter and Adelson, Edward},
	Date-Added = {2017-06-17 10:58:07 +0000},
	Date-Modified = {2017-06-17 10:58:22 +0000},
	Journal = {IEEE Transactions on communications},
	Keywords = {Background Tree},
	Number = {4},
	Pages = {532--540},
	Publisher = {IEEE},
	Title = {The Laplacian pyramid as a compact image code},
	Volume = {31},
	Year = {1983}}

@article{agarwal1998surface,
	Author = {Agarwal, Pankaj K and Suri, Subhash},
	Date-Added = {2017-06-16 19:07:24 +0000},
	Date-Modified = {2017-06-18 14:26:38 +0000},
	Journal = {SIAM Journal on Computing},
	Keywords = {Background Content;NP-Hard;Reconstruction Condition;Background Computer Graphics},
	Number = {4},
	Pages = {1016--1035},
	Publisher = {SIAM},
	Title = {Surface approximation and geometric partitions},
	Volume = {27},
	Year = {1998}}

@article{KartalKoc2015marsreview,
	Abstract = {This paper introduces information-theoretic measure of complexity (ICOMP) criterion for model selection in multivariate adaptive regression splines (MARS) to tradeoff efficiently between how well the model fits the data and the model complexity. As is well known, MARS is a popular nonparametric regression technique used to study the nonlinear relationship between a response variable and the set of predictors with the help of piecewise linear or cubic splines as basis functions. A critical aspect in determining the form of the nonparametric regression model during the MARS strategy is the evaluation of portfolio of submodels to select the best submodel with the appropriate number of knots over subset of predictors. In the usual regression modeling, when a large number of predictor variables are present in the model, and there is no precise information about the exact functional relationships among the variables, many model selection criteria still overfit the model. In this paper, to find the simplest model that balances the overfitting and underfitting for the model, ICOMP is proposed as a powerful model selection criterion for MARS modeling. Here, the model complexity is treated with respect to the interdependency of parameter estimates, as well as the number of free parameters in the model. We develop and study the performance of ICOMP along with several most popular model selection criteria such as Akaike's information criterion, Schwarz's Bayesian information criterion and generalized cross-validation in MARS modeling to select the best subset models. We provide two Monte Carlo simulation examples and a real benchmark example to demonstrate the utility and versatility of the proposed model selection approach to determine best functional form of the predictive model. Our numerical examples show that ICOMP provides a general model selection criterion with an insight to the interdependencies and/or correlational structure between parameter estimates in the selected model. This new approach can also be applicable to many complex statistical modeling problems.},
	Annote = {Provides a recent adaptation and review of the literature.},
	Author = {Kartal Koc, Elcin and Bozdogan, Hamparsum},
	Date-Added = {2017-06-16 18:17:35 +0000},
	Date-Modified = {2017-06-17 07:47:34 +0000},
	Doi = {10.1007/s10994-014-5440-5},
	Issn = {1573-0565},
	Journal = {Machine Learning},
	Keywords = {Background Statistics;Review;Resolution Function},
	Number = {1},
	Pages = {35--58},
	Title = {Model selection in multivariate adaptive regression splines (MARS) using information complexity as the fitness function},
	Url = {http://dx.doi.org/10.1007/s10994-014-5440-5},
	Volume = {101},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s10994-014-5440-5}}

@article{friedman1991multivariate,
	Annote = {Multi-variate adaptive regression spliens version of the 1989 approach

MARS},
	Author = {Friedman, Jerome H},
	Date-Added = {2017-06-16 18:07:39 +0000},
	Date-Modified = {2017-06-17 07:47:21 +0000},
	Journal = {The annals of statistics},
	Keywords = {Background Statistics;Resolution Function},
	Pages = {1--67},
	Publisher = {JSTOR},
	Title = {Multivariate adaptive regression splines},
	Year = {1991}}

@article{kohler2014review,
	Annote = {Reviews the work on Kernel Regression Estimators

Seems like the all follow similar schemes, nothing like what I am doing exactly. },
	Author = {K{\"o}hler, Max and Schindler, Anja and Sperlich, Stefan},
	Date-Added = {2017-06-16 17:59:46 +0000},
	Date-Modified = {2017-06-16 17:59:57 +0000},
	Journal = {International Statistical Review},
	Keywords = {Review; Background Statistics},
	Number = {2},
	Pages = {243--274},
	Publisher = {Wiley Online Library},
	Title = {A review and comparison of bandwidth selection methods for kernel regression},
	Volume = {82},
	Year = {2014}}

@article{brockmann1993locally,
	Annote = {From Donoho, as a adaptive variable kernel width method. 

Has a bandwith b(t), comparable to my resolution function. produces an iterative approach based on assymptotic results for minimization of the MSE.

Only adjust the bandwidth or R(y), not the particle locations. },
	Author = {Brockmann, Michael and Gasser, Theo and Herrmann, Eva},
	Date-Added = {2017-06-16 14:31:59 +0000},
	Date-Modified = {2017-06-17 07:47:44 +0000},
	Journal = {Journal of the American Statistical Association},
	Keywords = {Background Statistics;Resolution Function},
	Number = {424},
	Pages = {1302--1309},
	Publisher = {Taylor \& Francis},
	Title = {Locally adaptive bandwidth choice for kernel regression estimators},
	Volume = {88},
	Year = {1993}}

@article{friedman1989flexible,
	Annote = {Reference from Donoho, as a adaptive splines representation method.

Placing knots for approximation.

Aprently scheme is expensive, iterative added, checking local approximation of the the function approximation. 

(attempts to minimize MSE and uses GRV cost function or regurlazer.)},
	Author = {Friedman, Jerome H and Silverman, Bernard W},
	Date-Added = {2017-06-16 14:12:13 +0000},
	Date-Modified = {2017-06-16 14:29:08 +0000},
	Journal = {Technometrics},
	Keywords = {Background Statistics},
	Number = {1},
	Pages = {3--21},
	Publisher = {Taylor \& Francis},
	Title = {Flexible parsimonious smoothing and additive modeling},
	Volume = {31},
	Year = {1989}}

@book{breiman1984classification,
	Annote = {Paper cited by Donoho and Review, for tree based representations for piece-wise constant signal representations.

Does classification with trees, usually either with some iterative procedure, or also, using pruning methods. Says splitting rules are costly and over grow the trees.

The pruning methods has in some sense a similarity with the pulling shceme contrusction. },
	Author = {Breiman, Leo and Friedman, Jerome and Stone, Charles J and Olshen, Richard A},
	Date-Added = {2017-06-16 13:37:00 +0000},
	Date-Modified = {2017-06-16 13:58:18 +0000},
	Keywords = {Background Tree;Background Statistics},
	Publisher = {CRC press},
	Title = {Classification and regression trees},
	Year = {1984}}

@article{mikut2013automated,
	Author = {Mikut, Ralf and Dickmeis, Thomas and Driever, Wolfgang and Geurts, Pierre and Hamprecht, Fred A and Kausler, Bernhard X and Ledesma-Carbayo, Mar{\'\i}a J and Mar{\'e}e, Rapha{\"e}l and Mikula, Karol and Pantazis, Periklis and others},
	Date-Added = {2017-06-16 13:31:55 +0000},
	Date-Modified = {2017-06-16 13:32:23 +0000},
	Journal = {Zebrafish},
	Keywords = {Background SPIM Processing;Review},
	Number = {3},
	Pages = {401--421},
	Publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
	Title = {Automated processing of zebrafish imaging data: a survey},
	Volume = {10},
	Year = {2013}}

@article{coifman1992entropy,
	Annote = {Function representation by wavelet packet library, using tree structures.

The nodes of the trees represent different areas or entropy regions.

Has complexity, O(Nlog(N)) local trignometric is O(Nlog(N)^2).

},
	Author = {Coifman, Ronald R and Wickerhauser, M Victor},
	Date-Added = {2017-06-16 13:15:15 +0000},
	Date-Modified = {2017-06-16 13:23:07 +0000},
	Journal = {IEEE Transactions on information theory},
	Keywords = {Background Tree;Background Statistics},
	Number = {2},
	Pages = {713--718},
	Publisher = {IEEE},
	Title = {Entropy-based algorithms for best basis selection},
	Volume = {38},
	Year = {1992}}

@article{peyre2011review,
	Author = {Peyr{\'e}, Gabriel},
	Date-Added = {2017-06-16 12:59:54 +0000},
	Date-Modified = {2017-06-16 13:32:35 +0000},
	Journal = {IEEE Journal of Selected Topics in Signal Processing},
	Keywords = {Background General;Background Wavelets;Background Content;Review},
	Number = {5},
	Pages = {896--911},
	Publisher = {IEEE},
	Title = {A review of adaptive image representations},
	Volume = {5},
	Year = {2011}}

@webpage{arrayfire2015,
	Annote = {The library I used for basic filtering operations on the GPU for generation of synthetic data. },
	Author = {ArrayFire},
	Date-Added = {2017-06-16 11:21:12 +0000},
	Date-Modified = {2017-06-16 11:21:27 +0000},
	Keywords = {library;Implimentation},
	Title = {ArrayFire Library},
	Url = {http://arrayfire.com/},
	Year = {2015},
	Bdsk-Url-1 = {http://arrayfire.com/}}

@book{saint1991elementary,
	Annote = {Gives the multi-dimensional taylor series approximation that I use to derive the Resolution Bound in the thesis. },
	Author = {Saint Raymond, Xavier},
	Date-Added = {2017-06-16 11:19:42 +0000},
	Date-Modified = {2017-06-16 11:20:16 +0000},
	Keywords = {Theory;APR},
	Publisher = {CRC Press},
	Title = {Elementary introduction to the theory of pseudodifferential operators},
	Volume = {3},
	Year = {1991}}

@article{unser1993b,
	Annote = {I use the smoothing b-splines from this code for smoothing and gradient estimation. They allow O(1) iterative algorithms with respect to the smoothing window. },
	Author = {Unser, Michael and Aldroubi, Akram and Eden, Murray},
	Date-Added = {2017-06-16 11:17:50 +0000},
	Date-Modified = {2017-06-16 11:18:05 +0000},
	Journal = {IEEE transactions on signal processing},
	Keywords = {Implimentation;Gradient},
	Number = {2},
	Pages = {834--848},
	Publisher = {IEEE},
	Title = {B-spline signal processing. II. Efficiency design and applications},
	Volume = {41},
	Year = {1993}}

@article{crow1984summed,
	Annote = {Describes summed area tables that I use to estimate the Local Intensity Scale /sigma},
	Author = {Crow, Franklin C},
	Date-Added = {2017-06-16 11:15:24 +0000},
	Date-Modified = {2017-06-16 11:16:45 +0000},
	Journal = {ACM SIGGRAPH computer graphics},
	Keywords = {Implimentation;Sigma},
	Number = {3},
	Pages = {207--212},
	Publisher = {ACM},
	Title = {Summed-area tables for texture mapping},
	Volume = {18},
	Year = {1984}}

@article{devore1992image,
	Annote = {Gives theoretical bounds for optimal convergence rates of compression schemes for images of a given Besov Spaces.

},
	Author = {DeVore, Ronald A and Jawerth, Bj{\"o}rn and Lucier, Bradley J},
	Date-Added = {2017-06-14 12:53:16 +0000},
	Date-Modified = {2017-07-08 13:39:07 +0000},
	Journal = {IEEE Transactions on information theory},
	Keywords = {Background Wavelets;Optimality},
	Number = {2},
	Pages = {719--746},
	Publisher = {IEEE},
	Title = {Image compression through wavelet transform coding},
	Volume = {38},
	Year = {1992}}

@article{donoho1994ideal,
	Annote = {Shows that for noisy function approximations, a wavelet based truncation procedure will produce an optimal representation in terms of convergence in $n$ the number of pixels or samples in the estimate. 

For normally distributed errors in each $l$. 

The convergence is within log(n) factor of the optimal.

Relies on the huristic that small co-efficients can be squashed.

A new inequality in multivariate normal decision theory which we call the oracle inequality shows that attained performance differs from ideal performance by at most a factor of approximately 2 log n, where n is the sample size. Moreover no estimator can give a better guarantee than this. Within the class of spatially adaptive procedures, RiskShrink is essentially optimal. Relying only on the data, it comes within a factor log2 n of the performance of piecewise polynomial and variable- knot spline methods equipped with an oracle. In contrast, it is unknown how or if piecewise polynomial methods could be made to function this well when denied access to an oracle and forced to rely on data alone.
},
	Author = {Donoho, David L and Johnstone, Iain M},
	Date-Added = {2017-06-14 11:16:25 +0000},
	Date-Modified = {2017-07-08 13:39:23 +0000},
	Journal = {biometrika},
	Keywords = {Background Wavelets;Optimality;Resolution Function},
	Pages = {425--455},
	Publisher = {JSTOR},
	Title = {Ideal spatial adaptation by wavelet shrinkage},
	Year = {1994}}
